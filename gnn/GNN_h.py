# %% [markdown]
# # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

# %%
# 0. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ pip –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–±–æ—Ä–∫–∏
import pathlib
import zipfile
from sklearn.metrics import classification_report, confusion_matrix
from torch_geometric.utils import add_self_loops
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data
import numpy as np
import random
import torch
from sklearn.model_selection import train_test_split
import pandas as pd
import shutil
import os
%pip install - U pip setuptools wheel

# 4. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —É—Ç–∏–ª–∏—Ç
%pip install - U scikit-learn pandas networkx matplotlib rich tqdm
%pip install graphlime tensorflow

%pip install torch == 2.6.0 torchvision torchaudio - -index-url https: // download.pytorch.org/whl/cu126
%pip install torch_geometric
%pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv - f https: // data.pyg.org/whl/torch-2.6.0+cu126.html


# %% [markdown]
# # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞

# %%
# @title üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ)

# –ü–∞–ø–∫–∞, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
data_dir = 'data/'


data_dir = 'data/'

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞
if os.path.exists(data_dir):
    # –ï—Å–ª–∏ –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —É–¥–∞–ª—è–µ–º –≤—Å–µ –µ—ë —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
    for root, dirs, files in os.walk(data_dir, topdown=False):
        for name in files:
            os.remove(os.path.join(root, name))
        for name in dirs:
            os.rmdir(os.path.join(root, name))
else:
    # –ï—Å–ª–∏ –ø–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ—ë
    os.makedirs(data_dir)


# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –≤ –ø–∞–ø–∫—É data
!git clone https: // github.com/salam-ammari/Labeled-Transactions-based-Dataset-of-Ethereum-Network.git {data_dir}


zip_path = "data/Dataset.zip"
extract_path = pathlib.Path("data/unpacked")
with zipfile.ZipFile(zip_path) as zf:
    zf.extractall(extract_path)
print("‚úÖ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

# %%
# @title üßπ –ß—Ç–µ–Ω–∏–µ CSV
dataset_csv_path = extract_path / "Dataset" / "Dataset.csv"
df = pd.read_csv(dataset_csv_path)
print(df.shape, "—Å—Ç—Ä–æ–∫")
print(df.columns.tolist())

# %% [markdown]
# # –ò–Ω–∂–µ–Ω–µ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö

# %%
# %% [data scrubbing]
# –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï —Ö–æ—Ç–∏–º –¥–æ–ø—É—Å–∫–∞—Ç—å NaN
cols_required = [col for col in df.columns
                 if col not in ['from_category', 'to_category']]

# –°—á–∏—Ç–∞–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –ø–æ —ç—Ç–∏–º —Å—Ç–æ–ª–±—Ü–∞–º
missing_counts_req = df[cols_required].isnull().sum()
print("Missing values in required columns:\n", missing_counts_req)

# –î—Ä–æ–ø–∞–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –µ—Å—Ç—å —Ö–æ—Ç—å –æ–¥–Ω–æ NaN –≤ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö
df = df.dropna(subset=cols_required)
print("After dropping rows with missing in required cols:", df.shape)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –¥—Ä–æ–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
dup_count = df.duplicated().sum()
print("Duplicate rows count:", dup_count)
if dup_count > 0:
    df = df.drop_duplicates()
    print("Dropped duplicate rows. New shape:", df.shape)
else:
    print("No duplicate rows found.")

# %%
# –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ 0 –∏ 1 –≤ from_scam / to_scam
from_counts = df['from_scam'].value_counts().rename({0: 'non_scam', 1: 'scam'})
to_counts = df['to_scam'].value_counts().rename({0: 'non_scam', 1: 'scam'})

print("Sender (from_scam):")
print(from_counts)
print("\nRecipient (to_scam):")
print(to_counts)

# –ï—Å–ª–∏ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –≥–¥–µ –µ—Å—Ç—å —Ö–æ—Ç—å –æ–¥–Ω–∞ –ø–æ–º–µ—Ç–∫–∞ scam:
df['any_scam'] = ((df['from_scam'] == 1) | (df['to_scam'] == 1)).astype(int)
any_counts = df['any_scam'].value_counts().rename(
    {0: 'no_scam', 1: 'has_scam'})
print("\nTransactions with any scam flag:")
print(any_counts)

# %%

# # –î–æ–ø—É—Å—Ç–∏–º, df —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ—á–∏—â–µ–Ω –æ—Ç NaN/–¥—É–±–ª–∏–∫–∞—Ç–æ–≤

# # 1) –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ from_scam
# n_from_scam = df['from_scam'].sum()  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 2617
# df_from_pos = df[df['from_scam'] == 1]
# df_from_neg = df[df['from_scam'] == 0].sample(n=n_from_scam, random_state=42)
# df_balanced_from = pd.concat([df_from_pos, df_from_neg]) \
#                      .sample(frac=1, random_state=42) \
#                      .reset_index(drop=True)

# print("Balanced from_scam:")
# print(df_balanced_from['from_scam'].value_counts())

# # 2) –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ to_scam
# n_to_scam = df['to_scam'].sum()  # 11638
# df_to_pos = df[df['to_scam'] == 1]
# df_to_neg = df[df['to_scam'] == 0].sample(n=n_to_scam, random_state=42)
# df_balanced_to = pd.concat([df_to_pos, df_to_neg]) \
#                    .sample(frac=1, random_state=42) \
#                    .reset_index(drop=True)

# print("\nBalanced to_scam:")
# print(df_balanced_to['to_scam'].value_counts())

# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Ä–æ–≤–Ω–æ 2617 –¥–ª—è –æ–±–æ–∏—Ö (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∏—Å–ª–∞):
# desired_n = 2617

# # –ü–æ from_scam
# df_from_pos = df[df['from_scam'] == 1]
# df_from_neg = df[df['from_scam'] == 0].sample(n=desired_n, random_state=42)
# df_sampled_from_exact = pd.concat([df_from_pos, df_from_neg]) \
#                            .sample(frac=1, random_state=42) \
#                            .reset_index(drop=True)

# # –ü–æ to_scam
# df_to_pos = df[df['to_scam'] == 1].sample(n=desired_n, random_state=42)
# df_to_neg = df[df['to_scam'] == 0].sample(n=desired_n, random_state=42)
# df_sampled_to_exact = pd.concat([df_to_pos, df_to_neg]) \
#                          .sample(frac=1, random_state=42) \
#                          .reset_index(drop=True)

# print("\nExact 2617 each for from_scam:")
# print(df_sampled_from_exact['from_scam'].value_counts())
# print("\nExact 2617 each for to_scam:")
# print(df_sampled_to_exact['to_scam'].value_counts())


# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ df —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ—á–∏—â–µ–Ω –æ—Ç NaN/–¥—É–±–ª–∏–∫–∞—Ç–æ–≤
desired_n = 2617

# 1) –ë–∞–ª–∞–Ω—Å –ø–æ from_scam
pos_from = df[df["from_scam"] == 1]
neg_from = df[df["from_scam"] == 0].sample(n=desired_n, random_state=42)

df_from_balanced = (
    pd.concat([pos_from, neg_from])
      .sample(frac=1, random_state=42)
      .reset_index(drop=True)
)

print("FROM balance:\n", df_from_balanced["from_scam"].value_counts())

# 2) –ë–∞–ª–∞–Ω—Å –ø–æ to_scam –±–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π —Å df_from_balanced
pos_to = df[df["to_scam"] == 1].sample(n=desired_n, random_state=42)

# –∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ negative-to –∏—Å–∫–ª—é—á–∞–µ–º —É–∂–µ –≤–∑—è—Ç—ã–µ –≤ df_from_balanced
neg_to_candidates = df[df["to_scam"] == 0]
neg_to_candidates = neg_to_candidates.loc[
    ~neg_to_candidates.index.isin(df_from_balanced.index)
]

neg_to = neg_to_candidates.sample(n=desired_n, random_state=42)

df_to_balanced = (
    pd.concat([pos_to, neg_to])
      .sample(frac=1, random_state=42)
      .reset_index(drop=True)
)

print("\nTO balance:\n", df_to_balanced["to_scam"].value_counts())

# 3) –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –∏–Ω–¥–µ–∫—Å–æ–≤ –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–±–æ—Ä–∞–º–∏
intersection = set(df_from_balanced.index) & set(df_to_balanced.index)
print("\n–ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤:", intersection)

# 4) –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –µ–¥–∏–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
df_balanced = (
    pd.concat([df_from_balanced, df_to_balanced])
      .drop_duplicates()
      .reset_index(drop=True)
)
print("\n–ò—Ç–æ–≥–æ–≤—ã–π —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç:", df_balanced.shape)

df = df_balanced

# %%
# %% [feature engineering]

# 0) –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ df —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ—á–∏—â–µ–Ω –æ—Ç NaN/–¥—É–±–ª–∏–∫–∞—Ç–æ–≤

# 1) –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ–¥ –∏–∑ ISO-—Å—Ç—Ä–æ–∫–∏ block_timestamp
df['block_year'] = df['block_timestamp'].str[:4].astype(int)

# 2) –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ value (3 –∫–≤–∞–Ω—Ç–∏–ª—è)
df['value_cat'] = pd.qcut(df['value'], q=3, labels=False)

# 3) –°—á–∏—Ç–∞–µ–º node-–ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∞–¥—Ä–µ—Å–∞
# -------------------------------------------------------------------
# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∞–¥—Ä–µ—Å ‚Üí –≥–æ–¥ –ø–µ—Ä–≤–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
addr_years = pd.concat([
    df[['from_address', 'block_year']].rename(
        columns={'from_address': 'address'}),
    df[['to_address',   'block_year']].rename(
        columns={'to_address':   'address'}),
], ignore_index=True)
reg_year = addr_years.groupby('address')['block_year'].min().rename('reg_year')

# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∞–¥—Ä–µ—Å ‚Üí –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
addr_vals = pd.concat([
    df[['from_address', 'value']].rename(columns={'from_address': 'address'}),
    df[['to_address',   'value']].rename(columns={'to_address':   'address'}),
], ignore_index=True)
max_value = addr_vals.groupby('address')['value'].max().rename('max_value')

# –°–∫–ª–µ–∏–≤–∞–µ–º node-—Ç–∞–±–ª–∏—Ü—É
node_features_df = (
    pd.concat([reg_year, max_value], axis=1)
      .reset_index()               # index ‚Üí address
)
node_features_df.columns = ['address', 'reg_year', 'max_value']

# 4) –°—á–∏—Ç–∞–µ–º edge-–ø—Ä–∏–∑–Ω–∞–∫–∏, –º–µ—Ä–¥–∂–∏–º —Ç—É–¥–∞ –≥–æ–¥—ã —Ä–∞–±–æ—Ç—ã –∏–∑ node-—Ç–∞–±–ª–∏—Ü—ã
# -------------------------------------------------------------------
# –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–∏–º ¬´from_¬ª –ø—Ä–∏–∑–Ω–∞–∫–∏
edge_df = df[[
    'from_address', 'to_address',
    'gas', 'gas_price',
    'receipt_cumulative_gas_used', 'receipt_gas_used',
    'block_number', 'value_cat',
    'from_scam', 'to_scam'
]].copy()

# –ø—Ä–∏—Ü–µ–ø–ª—è–µ–º –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∏ max_value, –∑–∞—Ç–µ–º —Å—á–∏—Ç–∞–µ–º years_in_operation
edge_df = edge_df.merge(
    node_features_df.rename(columns={
        'address': 'from_address',
        'reg_year': 'from_reg_year',
        'max_value': 'from_max_value'
    }),
    on='from_address', how='left'
)
edge_df = edge_df.merge(
    node_features_df.rename(columns={
        'address': 'to_address',
        'reg_year': 'to_reg_year',
        'max_value': 'to_max_value'
    }),
    on='to_address', how='left'
)

current_year = edge_df['block_year'].max(
) if 'block_year' in edge_df else df['block_year'].max()
# –ù–æ –Ω–∞–º –Ω—É–∂–µ–Ω block_year –≤ edge_df –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ª–µ—Ç –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏:
edge_df['block_year'] = df['block_year']  # –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º
edge_df['from_years_in_operation'] = current_year - edge_df['from_reg_year']
edge_df['to_years_in_operation'] = current_year - edge_df['to_reg_year']

# 5) –ò—Ç–æ–≥–æ–≤—ã–µ —Å–ø–∏—Å–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
edge_feature_cols = [
    'gas', 'gas_price',
    'receipt_cumulative_gas_used', 'receipt_gas_used',
    'block_number', 'value_cat',
    'from_years_in_operation', 'to_years_in_operation'
]

node_feature_cols = [
    'address',    # ID —É–∑–ª–∞
    'reg_year',   # –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    'max_value',  # –º–∞–∫—Å. —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
]

response_cols = ['from_scam', 'to_scam']

# 6) –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—Å—ë –Ω–∞ –º–µ—Å—Ç–µ
print("Edge DataFrame shape:", edge_df.shape)
print("Edge feature columns:", edge_feature_cols)
print("Node DataFrame shape:", node_features_df.shape)
print("Node feature columns:", node_feature_cols)
print("Response columns:", response_cols)

# –ü—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –æ—Ç–¥–µ–ª–∏—Ç—å —á–∏—Å—Ç–æ –¥–≤–µ —Ç–∞–±–ª–∏—Ü—ã:
node_df = node_features_df.set_index('address')
edge_df = edge_df[['from_address', 'to_address'] +
                  edge_feature_cols + response_cols]

# %% [markdown]
# # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å

# %%
print("DataFrame columns:", df.columns.tolist())

# %%
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# 1) –º–µ—Ç–∫–∞ ¬´—Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –æ—Ç–º–µ—Ç–∫–∞ scam —É —É–∑–ª–∞¬ª
from_label = df.groupby("from_address")["from_scam"].max()
to_label = df.groupby("to_address")["to_scam"].max()

node_label = (
    from_label.reindex(node_df.index, fill_value=0).astype(int)
    | to_label.reindex(node_df.index, fill_value=0).astype(int)
)

# 2) train / val / test = 50‚ÄØ/‚ÄØ25‚ÄØ/‚ÄØ25 %, —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ
train_ids, rest_ids = train_test_split(
    node_df.index,
    train_size=0.5,
    stratify=node_label,
    random_state=SEED,
)
val_ids, test_ids = train_test_split(
    rest_ids,
    train_size=0.5,
    stratify=node_label.reindex(rest_ids),
    random_state=SEED,
)

print(
    f"train / val / test sizes: {len(train_ids)} | {len(val_ids)} | {len(test_ids)}")

# %%


# 1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≥—Ä–∞—Ñ–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#   node_df     ‚Äì –∫–∞–∫ –≤ –≤–∞—à–µ–º –ø–∞–π–ø–ª–∞–π–Ω–µ (index = address)
#   edge_df     ‚Äì —Å—Ç–æ–ª–±—Ü—ã ["from_address", "to_address", ‚Ä¶]
#   node_label  ‚Äì Series(address ‚Üí 0/1, ¬´scam‚Äë—É–∑–µ–ª¬ª)

addr2idx = {addr: i for i, addr in enumerate(node_df.index)}
num_nodes = len(addr2idx)

# Node‚Äë–ø—Ä–∏–∑–Ω–∞–∫–∏: –≤–æ–∑—å–º—ë–º –¥–≤–∞, —á—Ç–æ –≤—ã —É–∂–µ —Å–¥–µ–ª–∞–ª–∏ (reg_year, max_value)
x = torch.tensor(
    node_df[["reg_year", "max_value"]].values,
    dtype=torch.float32,
)

# –°–ø–∏—Å–æ–∫ —Ä—ë–±–µ—Ä (–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ, –∫–∞–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ)
edge_index = torch.tensor(
    [
        [addr2idx[s], addr2idx[t]]
        for s, t in zip(edge_df["from_address"], edge_df["to_address"])
        if s in addr2idx and t in addr2idx
    ],
    dtype=torch.long,
).t().contiguous()

# –î–æ–±–∞–≤–∏–º self‚Äëloops (–ø–æ–º–æ–≥–∞–µ—Ç —Å—Ö–æ–¥–∏—Ç—å—Å—è GraphSAGE)
edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)

# –ú–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫ —É–∑–ª–æ–≤
y = torch.tensor(node_label.values, dtype=torch.long)

# ‚îÄ Split –Ω–∞ train / val / test (–∞–¥—Ä–µ—Å–∞ —É –≤–∞—Å —É–∂–µ –≤—ã–±—Ä–∞–Ω—ã) ‚îÄ‚îÄ‚îÄ‚îÄ


def mask_from_ids(id_list):
    m = torch.zeros(num_nodes, dtype=torch.bool)
    m[[addr2idx[a] for a in id_list]] = True
    return m


train_mask = mask_from_ids(train_ids)
val_mask = mask_from_ids(val_ids)
test_mask = mask_from_ids(test_ids)

data = Data(
    x=x, edge_index=edge_index, y=y,
    train_mask=train_mask, val_mask=val_mask, test_mask=test_mask,
)

# 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


class GraphSAGE(torch.nn.Module):
    def __init__(self, in_ch, dropout=0.5):
        super().__init__()
        self.convs = torch.nn.ModuleList([
            SAGEConv(in_ch, 4),
            SAGEConv(4, 32, aggr="mean"),
            SAGEConv(32, 16, aggr="mean"),
            SAGEConv(16, 16, aggr="mean"),
            SAGEConv(16, 16, aggr="mean"),
            SAGEConv(16,  4, aggr="mean"),
        ])
        self.lin = torch.nn.Linear(4, 2)          # 2 –∫–ª–∞—Å—Å–∞
        self.dropout = dropout

    def forward(self, x, edge_index):
        for conv in self.convs:
            x = conv(x, edge_index)
            x = torch.relu(x)
            x = torch.nn.functional.dropout(
                x, p=self.dropout, training=self.training
            )
        return torch.log_softmax(self.lin(x), dim=-1)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GraphSAGE(in_ch=data.num_features).to(device)
data = data.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)
loss_fn = torch.nn.NLLLoss()

print(model)

# %% [markdown]
# # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

# %%
# 3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –û–±—É—á–µ–Ω–∏–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
EPOCHS = 50
for epoch in range(1, EPOCHS + 1):
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    # –≤–∞–ª–∏–¥–∞—Ü–∏—è
    model.eval()
    with torch.no_grad():
        pred = out.argmax(dim=1)
        train_acc = (pred[data.train_mask] ==
                     data.y[data.train_mask]).float().mean().item()
        val_acc = (pred[data.val_mask] == data.y[data.val_mask]
                   ).float().mean().item()

    if epoch == 1 or epoch % 5 == 0:
        print(f"Epoch {epoch:02d} | loss {loss:.4f} | "
              f"train_acc {train_acc:.4f} | val_acc {val_acc:.4f}")

# 4) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –¢–µ—Å—Ç + –æ—Ç—á—ë—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
model.eval()
with torch.no_grad():
    out = model(data.x, data.edge_index)
    pred = out.argmax(dim=1)
    test_acc = (pred[data.test_mask] ==
                data.y[data.test_mask]).float().mean().item()

print(f"\nTEST accuracy: {test_acc:.4f}")
print("\nClassification report:")
print(
    classification_report(
        data.y[data.test_mask].cpu(), pred[data.test_mask].cpu(), digits=4
    )
)
print("Confusion matrix:\n",
      confusion_matrix(
          data.y[data.test_mask].cpu(), pred[data.test_mask].cpu()
      ))
