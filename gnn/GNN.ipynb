{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzkv_AtRRyq0",
        "outputId": "7d9fe6eb-b8a6-4b93-8f94-e9ee82e47ed2"
      },
      "outputs": [],
      "source": [
        "# ===== 1. –°–Ω–æ—Å–∏–º –Ω–µ-–ø–æ–¥—Ö–æ–¥—è—â–∏–µ –≤–µ—Ä—Å–∏–∏ (–µ—Å–ª–∏ —Å—Ç–∞–≤–∏–ª–∏—Å—å —Ä–∞–Ω–µ–µ) =====\n",
        "# %pip uninstall -y torch torchvision torchaudio torchtext torchdata pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv || true\n",
        "\n",
        "# ===== 2. –°—Ç–∞–≤–∏–º PyTorch 2.4.1 (—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è, —Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è —Å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π) =====\n",
        "%pip install -q torch==2.4.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# ===== 3. –°—Ç–∞–≤–∏–º PyTorch Geometric 2.2.0 –∏ –∑–∞–≤–∏—Å–∏–º—ã–µ –∫–æ–ª—ë—Å–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ =====\n",
        "%pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
        "#%pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.4.1+cpu.html\n",
        "%pip install -q torch-geometric==2.2.0\n",
        "\n",
        "# ===== 4. –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ =====\n",
        "%pip install -q pandas scikit-learn networkx matplotlib rich tqdm\n",
        "%pip install -q graphlime\n",
        "%pip install focal-loss-torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, random, numpy as np\n",
        "import datetime\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, recall_score, confusion_matrix\n",
        "import torch.optim as optim\n",
        "from torch_geometric.utils import add_self_loops\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
        "import networkx as nx\n",
        "from torch_geometric.explain.algorithm.gnn_explainer import GNNExplainer_\n",
        "import joblib\n",
        "from collections import OrderedDict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHQvmJqsSFIs"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl2fzosaSboN",
        "outputId": "d2a5b2c1-b7d5-4e9b-febf-63b4f8fdefd8"
      },
      "outputs": [],
      "source": [
        "# print(\"Python           :\", sys.version.split()[0])\n",
        "# print(\"Torch            :\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "# print(\"PyG              :\", torch_geometric.__version__)\n",
        "# print(\"torch_scatter    :\", torch_scatter.__version__)\n",
        "# print(\"pandas           :\", pd.__version__)\n",
        "# print(\"scikit-learn     :\", sklearn.__version__)\n",
        "# print(\"networkx         :\", nx.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d8caCYCTsD8",
        "outputId": "b34c9470-99a9-4886-f709-3e4fa883f574"
      },
      "outputs": [],
      "source": [
        "#@title üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ)\n",
        "import os\n",
        "\n",
        "# –ü–∞–ø–∫–∞, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π\n",
        "data_dir = 'data/'\n",
        "\n",
        "# –ï—Å–ª–∏ –ø–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ—ë\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –≤ –ø–∞–ø–∫—É data\n",
        "!git clone https://github.com/salam-ammari/Labeled-Transactions-based-Dataset-of-Ethereum-Network.git {data_dir}\n",
        "\n",
        "\n",
        "import zipfile, pathlib\n",
        "zip_path = \"data/Dataset.zip\"\n",
        "extract_path = pathlib.Path(\"data/unpacked\")\n",
        "with zipfile.ZipFile(zip_path) as zf: zf.extractall(extract_path)\n",
        "print(\"‚úÖ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwxOMFriTtcO",
        "outputId": "1fc0d0cd-b07f-4710-8f07-ef7cc322fc7c"
      },
      "outputs": [],
      "source": [
        "#@title üßπ –ß—Ç–µ–Ω–∏–µ CSV\n",
        "import pandas as pd\n",
        "dataset_csv_path = extract_path / \"Dataset\" / \"Dataset.csv\"\n",
        "df = pd.read_csv(dataset_csv_path)\n",
        "print(df.shape, \"—Å—Ç—Ä–æ–∫\")\n",
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "yuq5b816UaNS",
        "outputId": "e28ddb31-8ebc-4f81-edc5-f68f7328fd8d"
      },
      "outputs": [],
      "source": [
        "# @title üßπ –≠—Ç–∞–ø 3 ‚Äî –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∞–∫—Ç—É–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è)\n",
        "\n",
        "# --- 0. —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è ---\n",
        "df_raw = df.copy()\n",
        "\n",
        "# --- 1. —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã ---\n",
        "num_cols = [\n",
        "    \"nonce\", \"transaction_index\", \"value\", \"gas\", \"gas_price\",\n",
        "    \"receipt_cumulative_gas_used\", \"receipt_gas_used\", \"block_number\"\n",
        "]\n",
        "for c in num_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\", downcast=\"integer\")\n",
        "\n",
        "# --- 2. —Ñ–ª–∞–≥–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ ---\n",
        "df[\"from_scam\"] = df[\"from_scam\"].astype(\"int8\")\n",
        "df[\"to_scam\"]   = df[\"to_scam\"].astype(\"int8\")\n",
        "\n",
        "# --- 3. –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–∞–∑–±–æ—Ä –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏ –±–ª–æ–∫–∞ ---\n",
        "# ‚ù∂ –∑–∞–º–µ–Ω—è–µ–º ' UTC' ‚Üí '+00:00' (–¥–µ–ª–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º)\n",
        "ts_fixed = df[\"block_timestamp\"].astype(str).str.replace(\" UTC\", \"+00:00\", regex=False)\n",
        "\n",
        "# ‚ù∑ –æ–¥–∏–Ω –≤—ã–∑–æ–≤ to_datetime, pandas ‚â• 2.0 —É–º–µ–µ—Ç format='mixed'\n",
        "df[\"block_timestamp\"] = pd.to_datetime(ts_fixed, utc=True, format=\"mixed\", errors=\"raise\")\n",
        "\n",
        "# --- 4. —É–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ –∞–¥—Ä–µ—Å–æ–≤ ---\n",
        "df.dropna(subset=[\"from_address\", \"to_address\"], inplace=True)\n",
        "\n",
        "# --- 5. —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ hash ---\n",
        "dup_before = len(df)\n",
        "df.drop_duplicates(subset=\"hash\", inplace=True)\n",
        "dup_removed = dup_before - len(df)\n",
        "\n",
        "# --- 6. —Å–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –∏—Å—Ö–æ–¥–Ω—ã–π ---\n",
        "df.reset_index(names=\"raw_row\", inplace=True)\n",
        "\n",
        "# --------- –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ ----------\n",
        "print(f\"‚û§ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(df)}  (–∏—Å—Ö–æ–¥–Ω–æ {len(df_raw)})\")\n",
        "print(f\"   —É–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {dup_removed}\")\n",
        "print(\"\\n–ú–µ—Ç–∫–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π:\")\n",
        "print(\"  from_scam =\", dict(df['from_scam'].value_counts()))\n",
        "print(\"  to_scam   =\", dict(df['to_scam'].value_counts()))\n",
        "print(\"\\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(df.dtypes.value_counts())\n",
        "print(\"\\n–ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏:\")\n",
        "display(df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "2o1__OilVgjW",
        "outputId": "890060c3-f816-4f99-d7ad-8bc413bdd38e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title –ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "# ---------- 1. –≤—Å–ø–æ–º-—Ñ—É–Ω–∫—Ü–∏–∏ ----------\n",
        "def log1p_clip(col):           # –ª–æ–≥–∞—Ä–∏—Ñ–º —Å–æ —Å–¥–≤–∏–≥–æ–º\n",
        "    return np.log1p(col).astype(\"float32\")\n",
        "\n",
        "# ---------- 2. –∞–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∞–¥—Ä–µ—Å–∞–º ----------\n",
        "g_from = (df\n",
        "          .groupby(\"from_address\")\n",
        "          .agg(tx_out_count = (\"hash\",  \"size\"),\n",
        "               tx_out_value = (\"value\", \"sum\"),\n",
        "               first_out_ts = (\"block_timestamp\", \"min\"),\n",
        "               last_out_ts  = (\"block_timestamp\", \"max\"))\n",
        "         )\n",
        "\n",
        "g_to = (df\n",
        "        .groupby(\"to_address\")\n",
        "        .agg(tx_in_count = (\"hash\",  \"size\"),\n",
        "             tx_in_value = (\"value\", \"sum\"),\n",
        "             first_in_ts = (\"block_timestamp\", \"min\"),\n",
        "             last_in_ts  = (\"block_timestamp\", \"max\"))\n",
        "       )\n",
        "\n",
        "nodes_df = g_from.join(g_to, how=\"outer\")\n",
        "\n",
        "# ---------- 3. –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∞–¥—Ä–µ—Å–Ω–æ ----------\n",
        "num_cols  = [\"tx_out_count\",\"tx_out_value\",\"tx_in_count\",\"tx_in_value\"]\n",
        "ts_cols   = [\"first_out_ts\",\"last_out_ts\",\"first_in_ts\",\"last_in_ts\"]\n",
        "\n",
        "nodes_df[num_cols] = nodes_df[num_cols].fillna(0).astype({\"tx_out_count\":\"int32\",\n",
        "                                                          \"tx_in_count\":\"int32\"})\n",
        "# –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ NaT ‚Äì —Å –Ω–∏–º–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç min/max\n",
        "\n",
        "# ---------- 4. years_active ----------\n",
        "first_ts = nodes_df[[\"first_out_ts\",\"first_in_ts\"]].min(axis=1, skipna=True)\n",
        "last_ts  = nodes_df[[\"last_out_ts\",\"last_in_ts\"]].max(axis=1, skipna=True)\n",
        "\n",
        "years = (last_ts - first_ts).dt.total_seconds() / (365*24*3600)\n",
        "nodes_df[\"years_active\"] = years.fillna(0).astype(\"float32\")\n",
        "\n",
        "# ---------- 5. –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä—É–µ–º —Å—É–º–º—ã ----------\n",
        "for col in [\"tx_out_value\",\"tx_in_value\"]:\n",
        "    nodes_df[col] = log1p_clip(nodes_df[col])\n",
        "\n",
        "# ---------- 6. –º–µ—Ç–∫–∞ scam-—É–∑–ª–∞ ----------\n",
        "scam_mask = pd.Series(0, index=nodes_df.index, dtype=\"int8\")\n",
        "scam_addr = pd.unique(pd.concat([df.loc[df[\"from_scam\"]==1,\"from_address\"],\n",
        "                                 df.loc[df[\"to_scam\"]==1,\"to_address\"]]))\n",
        "scam_mask.loc[scam_addr] = 1\n",
        "nodes_df[\"is_scam\"] = scam_mask\n",
        "\n",
        "# ---------- 7. —Ç–∞–±–ª–∏—Ü–∞ —Ä—ë–±–µ—Ä ----------\n",
        "edges_df = df[[\"from_address\",\"to_address\",\"value\",\"block_timestamp\"]].copy()\n",
        "edges_df.rename(columns={\"from_address\":\"source\",\n",
        "                         \"to_address\":\"target\"}, inplace=True)\n",
        "edges_df[\"log_value\"] = log1p_clip(edges_df[\"value\"])\n",
        "edges_df[\"ts_norm\"]   = edges_df[\"block_timestamp\"].astype(\"int64\") / 1e9\n",
        "edges_df.drop(columns=[\"value\",\"block_timestamp\"], inplace=True)\n",
        "\n",
        "# ---------- 8. –æ—Ç–ª–∞–¥–∫–∞ ----------\n",
        "print(f\"Nodes : {nodes_df.shape}\")\n",
        "print(f\"Edges : {edges_df.shape}\")\n",
        "print(\"\\nSample nodes_df:\")\n",
        "display(nodes_df.head(3))\n",
        "print(\"\\nSample edges_df:\")\n",
        "display(edges_df.head(3))\n",
        "print(\"\\nlabel distribution:\")\n",
        "print(nodes_df['is_scam'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM60LeRMWUfQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LAQLhQGZ_nw",
        "outputId": "40a90829-eb1f-49e3-f220-83f8173713d4"
      },
      "outputs": [],
      "source": [
        "# @title # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "addr2id = {addr: i for i, addr in enumerate(nodes_df.index)}\n",
        "num_nodes = len(addr2id)\n",
        "src = edges_df[\"source\"].map(addr2id).to_numpy()\n",
        "dst = edges_df[\"target\"].map(addr2id).to_numpy()\n",
        "edge_index = torch.tensor(np.vstack([src, dst]), dtype=torch.long)\n",
        "edge_attr = torch.tensor(edges_df[[\"log_value\", \"ts_norm\"]].to_numpy(dtype=np.float32), dtype=torch.float32)\n",
        "x = torch.tensor(nodes_df[[\"tx_out_count\", \"tx_out_value\", \"tx_in_count\", \"tx_in_value\", \"years_active\"]].to_numpy(dtype=np.float32), dtype=torch.float32)\n",
        "y = torch.tensor(nodes_df[\"is_scam\"].to_numpy(), dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7VBKQi0aXu8",
        "outputId": "a4b1bfe0-698c-4015-a614-5f962d11b0cf"
      },
      "outputs": [],
      "source": [
        "# @title # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ train, val, test\n",
        "train_idx, temp_idx = train_test_split(np.arange(num_nodes), test_size=0.5, stratify=y, random_state=SEED)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=y[temp_idx], random_state=SEED)\n",
        "def to_mask(idxs, size):\n",
        "    m = torch.zeros(size, dtype=torch.bool)\n",
        "    m[idxs] = True\n",
        "    return m\n",
        "\n",
        "train_mask = to_mask(train_idx, num_nodes)\n",
        "val_mask = to_mask(val_idx, num_nodes)\n",
        "test_mask = to_mask(test_idx, num_nodes)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RavI_k8Iz8O",
        "outputId": "076fd165-bfd1-4a01-9b09-375dc2b324eb"
      },
      "outputs": [],
      "source": [
        "# @title # –ú–æ–¥–µ–ª—å GNN\n",
        "class GNNModel(torch.nn.Module):\n",
        "    def __init__(self, in_feats):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(in_feats, 64)\n",
        "        self.conv2 = GCNConv(64, 32)\n",
        "        self.conv3 = GCNConv(32, 16)\n",
        "        self.conv4 = GCNConv(16, 2)  # Output layer\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = torch.relu(self.conv1(x, edge_index))\n",
        "        x = torch.dropout(x, p=0.2, train=self.training)  # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
        "        x = torch.relu(self.conv2(x, edge_index))\n",
        "        x = torch.dropout(x, p=0.2, train=self.training)\n",
        "        x = torch.relu(self.conv3(x, edge_index))\n",
        "        x = self.conv4(x, edge_index)  # Output logits\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GNNModel(in_feats=data.x.size(1)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å\n",
        "class_weights = torch.tensor([1.0, 10.0], device=device)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –¥–ª—è –∫–ª–∞—Å—Å–∞ –º–æ—à–µ–Ω–Ω–∏–∫–æ–≤\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π\n",
        "EPOCHS = 200\n",
        "best_rec = 0.0\n",
        "no_improve = 0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out_val = model(data.x, data.edge_index)\n",
        "        val_loss = criterion(out_val[data.val_mask], data.y[data.val_mask]).item()\n",
        "        val_pred = out_val[data.val_mask].argmax(dim=1).cpu()\n",
        "        val_true = data.y[data.val_mask].cpu()\n",
        "        val_rec = recall_score(val_true, val_pred, zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss:.4f} | Val Recall: {val_rec:.4f}\")\n",
        "\n",
        "    if val_rec > best_rec:\n",
        "        best_rec = val_rec\n",
        "        best_state = model.state_dict()\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= 3:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "model.load_state_dict(best_state)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out_test = model(data.x, data.edge_index)\n",
        "    test_pred = out_test[data.test_mask].argmax(dim=1).cpu()\n",
        "    test_true = data.y[data.test_mask].cpu()\n",
        "    test_rec = recall_score(test_true, test_pred, zero_division=0)\n",
        "    cm = confusion_matrix(test_true, test_pred)\n",
        "\n",
        "print(f\"Test Recall: {test_rec:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: ROC –∏ Precision-Recall\n",
        "fpr, tpr, _ = roc_curve(test_true, out_test[data.test_mask].softmax(dim=1)[:, 1].cpu())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "precision, recall, _ = precision_recall_curve(test_true, out_test[data.test_mask].softmax(dim=1)[:, 1].cpu())\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], '--', lw=1)\n",
        "plt.title('ROC Curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recall, precision, lw=2)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤\n",
        "artifacts_dir = Path(\"artifacts\")\n",
        "artifacts_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å (–≤–∫–ª—é—á–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –≤–µ—Å–∞)\n",
        "model_path = artifacts_dir / \"gnn_eth_fraud_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (x_cols)\n",
        "x_cols = [\"tx_out_count\", \"tx_out_value\", \"tx_in_count\", \"tx_in_value\", \"years_active\"]  # –ü—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –º–æ–¥–µ–ª–∏\n",
        "features_path = artifacts_dir / \"features_list.pkl\"\n",
        "with open(features_path, \"wb\") as f:\n",
        "    pickle.dump(x_cols, f)\n",
        "print(f\"–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {features_path}\")\n",
        "\n",
        "# –ï—Å–ª–∏ —É —Ç–µ–±—è –µ—Å—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, scaler –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏),\n",
        "# —Ç—ã –º–æ–∂–µ—à—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –µ–≥–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –≤ LSTM-–∫–æ–¥–µ.\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –¥–ª—è Scaler (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è):\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä —Å–∫–µ–π–ª–µ—Ä–∞\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(x.numpy())  # –ï—Å–ª–∏ —Ç–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –≤ numpy –º–∞—Å—Å–∏–≤–µ\n",
        "scaler_path = artifacts_dir / \"scaler.pkl\"\n",
        "with open(scaler_path, \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(f\"Scaler —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {scaler_path}\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤\n",
        "checkpoint_dir = artifacts_dir / \"checkpoints\"\n",
        "checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤ (—á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤)\n",
        "checkpoint_path = checkpoint_dir / \"gnn_best_weights.pth\"\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –≤–µ—Å–∞–º–∏\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "print(f\"–õ—É—á—à–∏–µ –≤–µ—Å–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {checkpoint_path}\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)\n",
        "# –î–ª—è GNN —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω—ã\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –≠—Ç–∞–ø 8 ‚Äî –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GNNExplainer\n",
        "explainer = GNNExplainer_(\n",
        "    model,\n",
        "    epochs=200,\n",
        "    lr=0.01,\n",
        "    return_type='log_prob',       # –ª–æ–≥-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
        "    feat_mask_type='feature',     # –º–∞—Å–∫–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    allow_edge_mask=True,         # –º–∞—Å–∫–∏—Ä—É–µ–º —Ä—ë–±—Ä–∞\n",
        ")\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —É–∑–µ–ª –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è (–ø–µ—Ä–≤—ã–π –º–æ—à–µ–Ω–Ω–∏–∫)\n",
        "node_idx = (data.y == 1).nonzero(as_tuple=False)[0].item()\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º –º–∞—Å–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏:\n",
        "node_feat_mask, edge_mask = explainer.explain_node(\n",
        "    node_idx,\n",
        "    x=data.x,\n",
        "    edge_index=data.edge_index,\n",
        ")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ —Ä—ë–±—Ä–∞\n",
        "threshold = float(edge_mask.mean().cpu())\n",
        "G = nx.Graph()\n",
        "src, dst = data.edge_index.cpu().numpy()\n",
        "for i, (u, v) in enumerate(zip(src, dst)):\n",
        "    if edge_mask[i] > threshold:\n",
        "        G.add_edge(int(u), int(v), weight=float(edge_mask[i].cpu()))\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw_networkx_nodes(G, pos, node_size=30)\n",
        "nx.draw_networkx_edges(\n",
        "    G, pos,\n",
        "    width=[d['weight']*5 for (_, _, d) in G.edges(data=True)],\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(f\"Important subgraph for node {node_idx}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "x_cols = [\"tx_out_count\", \"tx_out_value\", \"tx_in_count\", \"tx_in_value\", \"years_active\"]\n",
        "\n",
        "\n",
        "# –ü–µ—á–∞—Ç–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ —É–∑–ª–∞\n",
        "print(\"Feature importances:\")\n",
        "for feat_name, score in zip(x_cols, node_feat_mask.cpu()):\n",
        "    print(f\"  {feat_name}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import joblib, pandas as pd\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "X = nodes_df[[\"tx_out_count\", \"tx_out_value\", \"tx_in_count\", \"tx_in_value\", \"years_active\"]].values\n",
        "y = nodes_df[\"is_scam\"].values\n",
        "train_idx = data.train_mask.cpu().numpy()  # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
        "test_idx = data.test_mask.cpu().numpy()  # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏\n",
        "models = OrderedDict({\n",
        "    \"RF\": RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42),\n",
        "    \"DT\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\n",
        "})\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "scores = []\n",
        "\n",
        "# –û–±—É—á–∞–µ–º –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X[train_idx], y[train_idx])  # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "    y_hat = clf.predict(X[test_idx])  # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    p, r, f, _ = precision_recall_fscore_support(\n",
        "        y[test_idx], y_hat, zero_division=0, average=\"binary\"\n",
        "    )  # –û—Ü–µ–Ω–∫–∞ precision, recall, F1\n",
        "    scores.append((name, p, r, f))\n",
        "    print(f\"{name:3}  precision {p:.3f}  recall {r:.3f}  F1 {f:.3f}\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "baseline_df = pd.DataFrame(scores, columns=[\"model\", \"precision\", \"recall\", \"F1\"])\n",
        "print(\"\\nBaseline Models Comparison:\")\n",
        "print(baseline_df)\n",
        "\n",
        "# –¢–µ–ø–µ—Ä—å –æ—Ü–µ–Ω–∏–º –º–æ–¥–µ–ª—å GNN\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out_test = model(data.x, data.edge_index)  # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è GNN\n",
        "    test_pred = out_test[data.test_mask].argmax(dim=1).cpu()  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–ª–∞—Å—Å—ã\n",
        "    test_true = data.y[data.test_mask].cpu()  # –ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "    test_p, test_r, test_f, _ = precision_recall_fscore_support(\n",
        "        test_true, test_pred, zero_division=0, average=\"binary\"\n",
        "    )  # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ GNN\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏ GNN –≤ —Ç–∞–±–ª–∏—Ü—É\n",
        "gnn_results = {\n",
        "    \"model\": \"GNN\",\n",
        "    \"precision\": test_p,\n",
        "    \"recall\": test_r,\n",
        "    \"F1\": test_f\n",
        "}\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º pd.concat –≤–º–µ—Å—Ç–æ append\n",
        "gnn_results_df = pd.DataFrame([gnn_results])  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame\n",
        "baseline_df = pd.concat([baseline_df, gnn_results_df], ignore_index=True)\n",
        "\n",
        "# –í—ã–≤–æ–¥–∏–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "print(\"\\nComparison with GNN:\")\n",
        "print(baseline_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
