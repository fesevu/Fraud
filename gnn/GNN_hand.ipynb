{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4bb02a",
   "metadata": {},
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbb52b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3302.78s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (25.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (75.3.2)\n",
      "Requirement already satisfied: wheel in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3309.38s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch==2.4.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch==2.4.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch==2.4.1) (4.13.2)\n",
      "Requirement already satisfied: sympy in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch==2.4.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch==2.4.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch==2.4.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch==2.4.1) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from jinja2->torch==2.4.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from sympy->torch==2.4.1) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3315.18s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3322.88s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-scatter in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (2.1.2)\n",
      "Requirement already satisfied: torch-sparse in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (0.6.18)\n",
      "Requirement already satisfied: torch-cluster in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (1.6.3)\n",
      "Requirement already satisfied: torch-spline-conv in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-sparse) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from scipy->torch-sparse) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3328.66s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-geometric==2.2.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-geometric==2.2.0) (4.67.1)\n",
      "Requirement already satisfied: numpy in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-geometric==2.2.0) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-geometric==2.2.0) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-geometric==2.2.0) (3.1.6)\n",
      "Requirement already satisfied: requests in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-geometric==2.2.0) (2.32.3)\n",
      "Requirement already satisfied: pyparsing in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-geometric==2.2.0) (3.1.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-geometric==2.2.0) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from torch-geometric==2.2.0) (7.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from jinja2->torch-geometric==2.2.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from requests->torch-geometric==2.2.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from requests->torch-geometric==2.2.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from requests->torch-geometric==2.2.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from requests->torch-geometric==2.2.0) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from scikit-learn->torch-geometric==2.2.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from scikit-learn->torch-geometric==2.2.0) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3334.44s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: networkx in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (3.1)\n",
      "Requirement already satisfied: matplotlib in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: rich in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (14.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from rich) (4.13.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3341.40s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphlime in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: focal-loss-torch in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (0.1.2)\n",
      "Collecting stella\n",
      "  Using cached stella-0.1.0.tar.gz (13.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (2.13.0)\n",
      "Requirement already satisfied: torch in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from focal-loss-torch) (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from focal-loss-torch) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /Users/a1234/Fraud/.venv/lib/python3.8/site-packages (from stella) (4.67.1)\n",
      "Collecting astropy (from stella)\n",
      "  Using cached astropy-5.2.2-cp38-cp38-macosx_11_0_arm64.whl.metadata (8.2 kB)\n",
      "Collecting astroquery (from stella)\n",
      "  Using cached astroquery-0.4.7-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting sklearn (from stella)\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m‚ï∞‚îÄ>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m√ó\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 0. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ pip –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–±–æ—Ä–∫–∏\n",
    "%pip install -U pip setuptools wheel\n",
    "\n",
    "# 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch (CPU-–≤–µ—Ä—Å–∏—è)\n",
    "%pip install torch==2.4.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# 2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ typing-extensions\n",
    "%pip install -U typing-extensions\n",
    "\n",
    "# 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π PyTorch Geometric\n",
    "%pip install torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
    "%pip install torch-geometric==2.2.0\n",
    "\n",
    "# 4. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —É—Ç–∏–ª–∏—Ç\n",
    "%pip install -U scikit-learn pandas networkx matplotlib rich tqdm\n",
    "%pip install graphlime focal-loss-torch stella tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b59b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, recall_score, confusion_matrix\n",
    "import torch.optim as optim\n",
    "from torch_geometric.utils import add_self_loops\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import networkx as nx\n",
    "from torch_geometric.explain.algorithm.gnn_explainer import GNNExplainer_\n",
    "import joblib\n",
    "from collections import OrderedDict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import DirectedGraphSAGE\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from stellargraph.mapper import DirectedGraphSAGENodeGenerator\n",
    "from stellargraph.layer import DirectedGraphSAGE\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "from stellargraph.layer import GraphSAGE\n",
    "from stellargraph.layer.gcn import GCN\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489d9be",
   "metadata": {},
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4daa09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'data'...\n",
      "remote: Enumerating objects: 75, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 75 (delta 35), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (75/75), 31.54 MiB | 651.00 KiB/s, done.\n",
      "Resolving deltas: 100% (35/35), done.\n",
      "‚úÖ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "#@title üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ)\n",
    "import os\n",
    "\n",
    "# –ü–∞–ø–∫–∞, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π\n",
    "data_dir = 'data/'\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞\n",
    "if os.path.exists(data_dir):\n",
    "    # –ï—Å–ª–∏ –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —É–¥–∞–ª—è–µ–º –≤—Å–µ –µ—ë —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ\n",
    "    for root, dirs, files in os.walk(data_dir, topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "else:\n",
    "    # –ï—Å–ª–∏ –ø–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ—ë\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "\n",
    "# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –≤ –ø–∞–ø–∫—É data\n",
    "!git clone https://github.com/salam-ammari/Labeled-Transactions-based-Dataset-of-Ethereum-Network.git {data_dir}\n",
    "\n",
    "\n",
    "import zipfile, pathlib\n",
    "zip_path = \"data/Dataset.zip\"\n",
    "extract_path = pathlib.Path(\"data/unpacked\")\n",
    "with zipfile.ZipFile(zip_path) as zf: zf.extractall(extract_path)\n",
    "print(\"‚úÖ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97efc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71250, 18) —Å—Ç—Ä–æ–∫\n",
      "['hash', 'nonce', 'transaction_index', 'from_address', 'to_address', 'value', 'gas', 'gas_price', 'input', 'receipt_cumulative_gas_used', 'receipt_gas_used', 'block_timestamp', 'block_number', 'block_hash', 'from_scam', 'to_scam', 'from_category', 'to_category']\n"
     ]
    }
   ],
   "source": [
    "#@title üßπ –ß—Ç–µ–Ω–∏–µ CSV\n",
    "import pandas as pd\n",
    "dataset_csv_path = extract_path / \"Dataset\" / \"Dataset.csv\"\n",
    "df = pd.read_csv(dataset_csv_path)\n",
    "print(df.shape, \"—Å—Ç—Ä–æ–∫\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6288dc0",
   "metadata": {},
   "source": [
    "# –ò–Ω–∂–µ–Ω–µ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185a22b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " hash                               0\n",
      "nonce                              0\n",
      "transaction_index                  0\n",
      "from_address                       0\n",
      "to_address                         0\n",
      "value                              0\n",
      "gas                                0\n",
      "gas_price                          0\n",
      "input                              0\n",
      "receipt_cumulative_gas_used        0\n",
      "receipt_gas_used                   0\n",
      "block_timestamp                    0\n",
      "block_number                       0\n",
      "block_hash                         0\n",
      "from_scam                          0\n",
      "to_scam                            0\n",
      "from_category                  68622\n",
      "to_category                    59601\n",
      "dtype: int64\n",
      "Dropped rows with missing values. New shape: (27, 18)\n",
      "Duplicate rows count: 11\n",
      "Dropped duplicate rows. New shape: (16, 18)\n",
      "Dataframe after feature engineering: (16, 17)\n",
      "Edge features: ['gas', 'gas_price', 'receipt_cumulative_gas_used', 'receipt_gas_used', 'block_number', 'value_cat', 'from_years_in_operation', 'to_years_in_operation']\n",
      "Node features: ['from_reg_year', 'to_reg_year', 'from_max_value', 'to_max_value']\n",
      "Response columns: ['from_scam', 'to_scam']\n",
      "\n",
      "Preview of selected features at various positions:\n",
      "\n",
      "      gas     gas_price  receipt_cumulative_gas_used  receipt_gas_used  block_number  value_cat  from_years_in_operation  to_years_in_operation  from_reg_year  to_reg_year  from_max_value  to_max_value  from_scam  to_scam\n",
      "0   21000  6.100000e+10                       330593             21000       4767697          1                        2                      2           2017         2017    1.020000e+20  1.020000e+20          1        1\n",
      "8   21000  2.000000e+10                      2777100             21000       5511845          2                        1                      1           2018         2018    1.890000e+19  1.890000e+19          1        1\n",
      "15  21000  5.000000e+10                       220968             21000       7682123          0                        0                      0           2019         2019    1.000000e+14  1.000000e+14          1        1\n"
     ]
    }
   ],
   "source": [
    "# %% [data scrubbing]\n",
    "# Check for missing values and drop if any\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)\n",
    "if missing_counts.any():\n",
    "    df = df.dropna()\n",
    "    print(\"Dropped rows with missing values. New shape:\", df.shape)\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Check for duplicate rows and drop if any\n",
    "dup_count = df.duplicated().sum()\n",
    "print(\"Duplicate rows count:\", dup_count)\n",
    "if dup_count > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(\"Dropped duplicate rows. New shape:\", df.shape)\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "# %% [feature engineering]\n",
    "# Reload dataframe if block_timestamp column was dropped in a previous run\n",
    "if 'block_timestamp' not in df.columns:\n",
    "    df = pd.read_csv(dataset_csv_path)\n",
    "# Extract year directly from block_timestamp string\n",
    "df['block_year'] = df['block_timestamp'].str[:4].astype(int)\n",
    "\n",
    "# Categorize transaction values into 3 quantile-based categories\n",
    "df['value_cat'] = pd.qcut(df['value'], q=3, labels=False)\n",
    "\n",
    "# Compute registration year (first seen block year) per node address\n",
    "from_reg = df.groupby('from_address')['block_year'].min().rename('from_reg_year')\n",
    "to_reg   = df.groupby('to_address')['block_year'].min().rename('to_reg_year')\n",
    "df = df.merge(from_reg, on='from_address', how='left').merge(to_reg, on='to_address', how='left')\n",
    "\n",
    "# Compute years in operation based on the latest block year in the dataset\n",
    "current_year = df['block_year'].max()\n",
    "df['from_years_in_operation'] = current_year - df['from_reg_year']\n",
    "df['to_years_in_operation']   = current_year - df['to_reg_year']\n",
    "\n",
    "# Compute max transaction value per node address\n",
    "from_max = df.groupby('from_address')['value'].max().rename('from_max_value')\n",
    "to_max   = df.groupby('to_address')['value'].max().rename('to_max_value')\n",
    "df = df.merge(from_max, on='from_address', how='left').merge(to_max, on='to_address', how='left')\n",
    "\n",
    "# Drop unneeded columns\n",
    "cols_to_drop = [\n",
    "    'hash', 'nonce', 'transaction_index', 'input',\n",
    "    'block_timestamp', 'block_hash',\n",
    "    'from_category', 'to_category', 'value'\n",
    "]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Define feature sets\n",
    "edge_features   = [\n",
    "    'gas', 'gas_price', 'receipt_cumulative_gas_used', 'receipt_gas_used',\n",
    "    'block_number', 'value_cat', 'from_years_in_operation', 'to_years_in_operation'\n",
    "]\n",
    "node_features   = ['from_reg_year', 'to_reg_year', 'from_max_value', 'to_max_value']\n",
    "response_cols   = ['from_scam', 'to_scam']\n",
    "\n",
    "print(\"Dataframe after feature engineering:\", df.shape)\n",
    "print(\"Edge features:\", edge_features)\n",
    "print(\"Node features:\", node_features)\n",
    "print(\"Response columns:\", response_cols)\n",
    "\n",
    "# %% [data preview]\n",
    "# Show sample records from beginning, middle, and end for clarity\n",
    "sample_idxs = [0, df.shape[0] // 2, df.shape[0] - 1]\n",
    "preview_df = df.loc[sample_idxs, edge_features + node_features + response_cols]\n",
    "print(\"\\nPreview of selected features at various positions:\\n\")\n",
    "print(preview_df.to_string(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f224b60",
   "metadata": {},
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13bd29e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\\n GCN(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(4, 32)\n",
      "    (1): GCNConv(32, 16)\n",
      "    (2-3): 2 x GCNConv(16, 16)\n",
      "    (4): GCNConv(16, 4)\n",
      "    (5): GCNConv(4, 2)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n",
      "Total trainable parameters: 1310\n"
     ]
    }
   ],
   "source": [
    "# %% [define model]\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.convs.append(GCNConv(dims[i], dims[i+1]))\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "input_dim = len(node_features)\n",
    "hidden_dims = [32, 16, 16, 16, 4]\n",
    "output_dim = len(response_cols)\n",
    "model = GCN(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "# Debug information\n",
    "print(\"Model architecture:\\\\n\", model)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e33d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %% [set optimizer]\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Optimizer initialized:\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08469dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function defined: BCEWithLogitsLoss()\n"
     ]
    }
   ],
   "source": [
    "# %% [define loss]\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "print(\"Loss function defined:\", criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c5e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass on dummy data output shape: torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "# %% [debug forward]\n",
    "# Test model forward pass on dummy data\n",
    "dummy_nodes = 5\n",
    "dummy_edges = 10\n",
    "dummy_x = torch.randn(dummy_nodes, input_dim)\n",
    "# Random edge_index for dummy graph\n",
    "source = torch.randint(0, dummy_nodes, (dummy_edges,))\n",
    "target = torch.randint(0, dummy_nodes, (dummy_edges,))\n",
    "dummy_edge_index = torch.stack([source, target], dim=0)\n",
    "out = model(dummy_x, dummy_edge_index)\n",
    "print(\"Forward pass on dummy data output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f7cc9",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a83f6a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)       [(1, 18, 4)]                 0         []                            \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (1, 18, 4)                   0         ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)       [(1, 18, 18)]                0         []                            \n",
      "                                                                                                  \n",
      " graph_convolution_15 (Grap  (1, 18, 32)                  160       ['dropout_15[0][0]',          \n",
      " hConvolution)                                                       'input_12[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (1, 18, 32)                  0         ['graph_convolution_15[0][0]']\n",
      "                                                                                                  \n",
      " graph_convolution_16 (Grap  (1, 18, 16)                  528       ['dropout_16[0][0]',          \n",
      " hConvolution)                                                       'input_12[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (1, 18, 16)                  0         ['graph_convolution_16[0][0]']\n",
      "                                                                                                  \n",
      " graph_convolution_17 (Grap  (1, 18, 16)                  272       ['dropout_17[0][0]',          \n",
      " hConvolution)                                                       'input_12[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)        (1, 18, 16)                  0         ['graph_convolution_17[0][0]']\n",
      "                                                                                                  \n",
      " graph_convolution_18 (Grap  (1, 18, 16)                  272       ['dropout_18[0][0]',          \n",
      " hConvolution)                                                       'input_12[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)        (1, 18, 16)                  0         ['graph_convolution_18[0][0]']\n",
      "                                                                                                  \n",
      " graph_convolution_19 (Grap  (1, 18, 4)                   68        ['dropout_19[0][0]',          \n",
      " hConvolution)                                                       'input_12[0][0]']            \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)       [(1, None)]                  0         []                            \n",
      "                                                                                                  \n",
      " gather_indices_3 (GatherIn  (1, None, 4)                 0         ['graph_convolution_19[0][0]',\n",
      " dices)                                                              'input_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (1, None, 2)                 10        ['gather_indices_3[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1310 (5.12 KB)\n",
      "Trainable params: 1310 (5.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 - 1s - loss: 22.3063 - acc: 0.7500 - val_loss: 1.8032 - val_acc: 1.0000 - lr: 0.0050 - 856ms/epoch - 856ms/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 63.8187 - acc: 0.8750 - val_loss: 1.5634 - val_acc: 1.0000 - lr: 0.0050 - 24ms/epoch - 24ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 9.3200 - acc: 0.7500 - val_loss: 1.4937 - val_acc: 1.0000 - lr: 0.0050 - 25ms/epoch - 25ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 16.8537 - acc: 0.8750 - val_loss: 1.4536 - val_acc: 1.0000 - lr: 0.0050 - 29ms/epoch - 29ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 18.9153 - acc: 1.0000 - val_loss: 1.4455 - val_acc: 1.0000 - lr: 0.0050 - 24ms/epoch - 24ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 92.1843 - acc: 0.7500 - val_loss: 1.4357 - val_acc: 1.0000 - lr: 0.0050 - 23ms/epoch - 23ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 25.0199 - acc: 0.6250 - val_loss: 1.4241 - val_acc: 1.0000 - lr: 0.0050 - 24ms/epoch - 24ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 7.7207 - acc: 0.7500 - val_loss: 1.4093 - val_acc: 1.0000 - lr: 0.0050 - 24ms/epoch - 24ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 1.7372 - acc: 0.8750 - val_loss: 1.3942 - val_acc: 1.0000 - lr: 0.0050 - 24ms/epoch - 24ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 14.0733 - acc: 0.7500 - val_loss: 1.3938 - val_acc: 1.0000 - lr: 0.0050 - 24ms/epoch - 24ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 153.3718 - acc: 0.6250 - val_loss: 1.3938 - val_acc: 1.0000 - lr: 0.0050 - 24ms/epoch - 24ms/step\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 26.6734 - acc: 0.6250 - val_loss: 1.3948 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 5.5173 - acc: 1.0000 - val_loss: 1.3949 - val_acc: 1.0000 - lr: 0.0050 - 23ms/epoch - 23ms/step\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 41.5848 - acc: 0.6250 - val_loss: 1.3938 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 36.0419 - acc: 0.7500 - val_loss: 1.3929 - val_acc: 1.0000 - lr: 0.0050 - 23ms/epoch - 23ms/step\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 33.5196 - acc: 1.0000 - val_loss: 1.3919 - val_acc: 1.0000 - lr: 0.0050 - 23ms/epoch - 23ms/step\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 3.6019 - acc: 1.0000 - val_loss: 1.3905 - val_acc: 1.0000 - lr: 0.0050 - 24ms/epoch - 24ms/step\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 11.5044 - acc: 1.0000 - val_loss: 1.3896 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 4.1667 - acc: 1.0000 - val_loss: 1.3889 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 3.7560 - acc: 0.8750 - val_loss: 1.3883 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 63.2623 - acc: 1.0000 - val_loss: 1.3874 - val_acc: 1.0000 - lr: 0.0050 - 27ms/epoch - 27ms/step\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 37.2446 - acc: 1.0000 - val_loss: 1.3869 - val_acc: 1.0000 - lr: 0.0050 - 25ms/epoch - 25ms/step\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 65.2475 - acc: 1.0000 - val_loss: 1.3865 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 15.4814 - acc: 0.8750 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 4.1177 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0050 - 23ms/epoch - 23ms/step\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 13.3515 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 9.7467 - acc: 0.8750 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 31.0253 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0050 - 22ms/epoch - 22ms/step\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 1.3878 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0050 - 23ms/epoch - 23ms/step\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 141.4311 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0025 - 22ms/epoch - 22ms/step\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 14.1100 - acc: 0.8750 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0025 - 22ms/epoch - 22ms/step\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 1.4142 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0025 - 21ms/epoch - 21ms/step\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 5.2108 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0025 - 22ms/epoch - 22ms/step\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 38.6825 - acc: 0.7500 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0025 - 22ms/epoch - 22ms/step\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 18.6361 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 1.0000 - lr: 0.0012 - 22ms/epoch - 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 20.4813 - acc: 1.0000\n",
      "Test loss: 20.4813, Test accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 score: 0.0\n",
      "Confusion Matrix:\n",
      " [[5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a1234/Fraud/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/a1234/Fraud/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/a1234/Fraud/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "# %% [model training]\n",
    "import tensorflow as tf\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "\n",
    "# Define the full-batch generator with self-loops\n",
    "generator = FullBatchNodeGenerator(\n",
    "    G,\n",
    "    method=\"self_loops\",\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "# %% [Keras GCN model definition]\n",
    "from stellargraph.layer.gcn import GCN as StellarGCN\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Build and compile the StellarGraph GCN model\n",
    "layer_sizes = [32, 16, 16, 16, 4]\n",
    "gcn_keras = StellarGCN(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activations=[\"relu\"] * len(layer_sizes),\n",
    "    generator=generator,\n",
    "    dropout=0.5,\n",
    ")\n",
    "x_inp, x_out = gcn_keras.in_out_tensors()\n",
    "prediction = Dense(units=2, activation=\"softmax\")(x_out)\n",
    "model = Model(inputs=x_inp, outputs=prediction)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.005),\n",
    "    loss=categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "model.summary()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "# Prepare node labels by merging 'from_scam' and 'to_scam'\n",
    "node_labels_from = df.groupby('from_address')['from_scam'].max()\n",
    "node_labels_to   = df.groupby('to_address')['to_scam'].max()\n",
    "node_labels      = pd.concat([node_labels_from, node_labels_to], axis=1).max(axis=1)\n",
    "\n",
    "# One-hot encode labels\n",
    "node_targets = pd.get_dummies(node_labels.astype(int))\n",
    "\n",
    "# Stratified split into train, validation, and test sets using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 75% train+val, 25% test\n",
    "train_val_nodes, test_nodes = train_test_split(\n",
    "    node_labels.index,\n",
    "    test_size=0.25,\n",
    "    stratify=node_labels,\n",
    "    random_state=42\n",
    ")\n",
    "# Second split: of train_val into 2/3 train (50% total) and 1/3 val (25% total)\n",
    "train_nodes, val_nodes = train_test_split(\n",
    "    train_val_nodes,\n",
    "    test_size=1/3,\n",
    "    stratify=node_labels.loc[train_val_nodes],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_targets = node_targets.loc[train_nodes].values\n",
    "val_targets   = node_targets.loc[val_nodes].values\n",
    "test_targets  = node_targets.loc[test_nodes].values\n",
    "\n",
    "# Create data generators\n",
    "train_gen = generator.flow(train_nodes, train_targets)\n",
    "val_gen   = generator.flow(val_nodes,   val_targets)\n",
    "test_gen  = generator.flow(test_nodes,  test_targets)\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=100,\n",
    "    validation_data=val_gen,\n",
    "    verbose=2,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = model.evaluate(test_gen)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "# Get model predictions\n",
    "y_pred = model.predict(test_gen)\n",
    "\n",
    "# Squeeze out the batch dimension if present (full-batch sequence yields shape (1, N, classes))\n",
    "if isinstance(y_pred, np.ndarray) and y_pred.ndim == 3:\n",
    "    y_pred = y_pred[0]\n",
    "\n",
    "# Compute predicted classes and true labels\n",
    "preds = np.argmax(y_pred, axis=1)\n",
    "truths = np.argmax(test_targets, axis=1)\n",
    "\n",
    "# Print classification metrics\n",
    "print(\"Recall:\", recall_score(truths, preds))\n",
    "print(\"Precision:\", precision_score(truths, preds))\n",
    "print(\"F1 score:\", f1_score(truths, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(truths, preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
