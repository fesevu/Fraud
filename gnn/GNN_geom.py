# %%
%pip uninstall torch torch_geometric torch_scatter torch_sparse pyg_lib torch_cluster torch_spline_conv -y
# 2.1 PyTorch (CPU) ‚Äì –ø–æ–¥–º–µ–Ω–∏—Ç–µ –∏–Ω–¥–µ–∫—Å, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ CUDA
%pip install torch==2.3.0

# 2.2 PyG: —Å 2.3+ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –ø–æ—á—Ç–∏ –Ω–µ—Ç, —Å—Ç–∞–≤–∏–º –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π
%pip install torch_geometric
%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cpu.html


# 2.4 –æ—Å—Ç–∞–ª—å–Ω–æ–µ
%pip install xgboost pandas matplotlib graphviz scikit-learn tqdm numpy networkx seaborn


# %%
from pathlib import Path
import networkx as nx
import pandas as pd
import numpy as np
import torch
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx
from torch_geometric.nn import Node2Vec
from torch_geometric.utils import to_undirected
import math
import torch_cluster

print(torch.__version__)

# %%
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# %% [markdown]
# # –ß—Ç–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞

# %%
from pathlib import Path
ACCOUNTS_CSV = Path('./tmp/Ethereum/account.csv')      # –æ–±–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
TXS_CSV      = Path('./tmp/Ethereum/transaction.csv')   # –æ–±–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

assert ACCOUNTS_CSV.exists(), f'{ACCOUNTS_CSV} not found'
assert TXS_CSV.exists(), f'{TXS_CSV} not found'

# %%
acc_df = pd.read_csv(ACCOUNTS_CSV, header=None, names=['id', 'label'])
tx_df  = pd.read_csv(TXS_CSV, header=None, names=['src', 'dst', 'amount', 'timestamp'])

display(acc_df.head())
display(tx_df.head())

# %% [markdown]
# ## üèóÔ∏è –°–±–æ—Ä–∫–∞ –≥—Ä–∞—Ñ–∞

# %%
G = nx.DiGraph()

# add nodes
for _, row in acc_df.iterrows():
    G.add_node(row.id, label=int(bool(row.label)))

# add edges
for _, row in tx_df.iterrows():
    G.add_edge(row.src, row.dst, amount=float(row.amount), ts=float(row.timestamp))

print(f'–ì—Ä–∞—Ñ —Å–æ–¥–µ—Ä–∂–∏—Ç {G.number_of_nodes():,} —É–∑–ª–æ–≤ –∏ {G.number_of_edges():,} —Ä—ë–±–µ—Ä')

# %%
# –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
in_deg  = dict(G.in_degree())
out_deg = dict(G.out_degree())

# –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å—É–º–º—ã
sent_sum = {n: 0.0 for n in G.nodes()}
recv_sum = {n: 0.0 for n in G.nodes()}
for u, v, d in G.edges(data=True):
    amt = d['amount']
    sent_sum[u] += amt
    recv_sum[v] += amt

# PageRank –∏ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
pr = nx.pagerank(G, alpha=0.85)
clust = nx.clustering(G.to_undirected())

# –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∏—á–∏ –≤ –≥—Ä–∞—Ñ
for n in G.nodes():
    G.nodes[n].update({
        'in_deg':      in_deg.get(n, 0),
        'out_deg':     out_deg.get(n, 0),
        'sent_sum':    sent_sum[n],
        'recv_sum':    recv_sum[n],
        'net_sum':     sent_sum[n] - recv_sum[n],
        'pagerank':    pr[n],
        'clustering':  clust[n],
    })

# --- –ø–æ—Å–ª–µ —Ä–∞—Å—á—ë—Ç–∞ pr –∏ clust --------------------------------------------
print('‚è≥ Calculating extra graph features‚Ä¶')

# 1) Betweenness (approx)
btw = nx.betweenness_centrality(G, k=10_000, seed=42, normalized=True)

# 2) Weakly-connected component size
G_u = G.to_undirected()
wcc = {n: 0 for n in G}
for comp in nx.connected_components(G_u):
    size = len(comp)
    for n in comp:
        wcc[n] = size

# 3) Min amounts
send_min = {n: math.inf for n in G}
recv_min = {n: math.inf for n in G}
for u, v, d in G.edges(data=True):
    amt = d['amount']
    send_min[u] = min(send_min[u], amt)
    recv_min[v] = min(recv_min[v], amt)
# –∑–∞–º–µ–Ω—è–µ–º inf ‚Üí 0.0 (—É–∑–ª—ã –±–µ–∑ –æ–ø–µ—Ä–∞—Ü–∏–π)
send_min = {n: 0.0 if math.isinf(v) else v for n, v in send_min.items()}
recv_min = {n: 0.0 if math.isinf(v) else v for n, v in recv_min.items()}

# --- –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Å–µ –Ω–æ–≤—ã–µ —Ñ–∏—á–∏ –≤ –≤–µ—Ä—à–∏–Ω—ã ----------------------------------
for n in G.nodes():
    G.nodes[n].update({
        'btw_centr':   btw[n],
        'wcc_size':    wcc[n],
        'send_min':    send_min[n],
        'recv_min':    recv_min[n],
    })

# %%
 # --- –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º Node2Vec: —Å–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ id ‚Üí idx -------------------
id2idx = {n: i for i, n in enumerate(G.nodes())}

# --- —Å—Ç—Ä–æ–∏–º edge_index –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∏–Ω–¥–µ–∫—Å–∞–º ------------------------------
edges = list(G.edges())
row = [id2idx[u] for u, v in edges]
col = [id2idx[v] for u, v in edges]
edge_index = torch.tensor([row, col], dtype=torch.long)
edge_index = to_undirected(edge_index).contiguous()

# --- —Ç–µ–ø–µ—Ä—å –æ–±—É—á–∞–µ–º Node2Vec –Ω–∞ –Ω–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≥—Ä–∞—Ñ–µ --------------------
print('‚è≥ Training Node2Vec‚Ä¶')
n2v = Node2Vec(
    edge_index=edge_index,
    num_nodes=G.number_of_nodes(),
    embedding_dim=128,
    walk_length=20,
    context_size=10,
    walks_per_node=5,
    num_negative_samples=1,
    sparse=True,
    p=1, q=1
).to(device)

loader = n2v.loader(batch_size=1024, shuffle=True)
opt = torch.optim.SparseAdam(list(n2v.parameters()), lr=0.01)

n2v.train()
for epoch in range(25):  # –º–æ–∂–µ—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å —á–∏—Å–ª–æ —ç–ø–æ—Ö
    total_loss = 0
    for pos_rw, neg_rw in loader:
        opt.zero_grad()
        loss = n2v.loss(pos_rw.to(device), neg_rw.to(device))
        loss.backward()
        opt.step()
        total_loss += loss.item()
    print(f'Epoch {epoch+1}, Loss {total_loss:.4f}')

# %%
# --- –∏–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ G ------------------------
z = n2v.embedding.weight.detach().cpu()
for n, idx in id2idx.items():
    G.nodes[n]['n2v'] = z[idx]

# --- –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ data.x ---------------------------------------------
# –ù–∞–∫–æ–Ω–µ—Ü, —Å–æ–±–∏—Ä–∞–µ–º x
num_attr_keys = [
    'in_deg','out_deg','sent_sum','recv_sum','net_sum',
    'pagerank','clustering',
    'btw_centr','wcc_size','send_min','recv_min'
]

for n in G.nodes():
    # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å—ë –µ—Å—Ç—å
    missing = [k for k in num_attr_keys if k not in G.nodes[n]]
    if missing:
        raise RuntimeError(f"–£ —É–∑–ª–∞ {n} –Ω–µ—Ç —Ñ–∏—á–µ–π {missing}")
    base = [float(G.nodes[n][k]) for k in num_attr_keys]
    emb  = G.nodes[n]['n2v'].tolist()           # 128-–º–µ—Ä–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
    G.nodes[n]['x'] = torch.tensor(base + emb, dtype=torch.float)

print('–ü—Ä–∏–º–µ—Ä —Ñ–∏—á —É–∑–ª–∞:', list(G.nodes(data=True))[0])

# %%
# üîß –î–µ–ª–∞–µ–º —Ç–∞–∫, —á—Ç–æ–±—ã —É –≤—Å–µ—Ö —É–∑–ª–æ–≤ –±—ã–ª–∞ –º–µ—Ç–∫–∞
nx.set_node_attributes(G, -1, "label")        # —Å–Ω–∞—á–∞–ª–∞ –≤—Å–µ–º —Å—Ç–∞–≤–∏–º -1
for _, row in acc_df.iterrows():              # –∑–∞—Ç–µ–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º, —á—Ç–æ –µ—Å—Ç—å –≤ accounts.csv
    G.nodes[row.id]["label"] = int(bool(row.label))   

# %%
# –ø–æ—Å–ª–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –¥–æ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ PyG
from sklearn.preprocessing import StandardScaler
X = np.stack([G.nodes[n]['x'].numpy() for n in G.nodes()])
scaler = StandardScaler()
X = scaler.fit_transform(X)
for n, vec in zip(G.nodes(), X):
    G.nodes[n]['x'] = torch.tensor(vec, dtype=torch.float)

# %% [markdown]
# ## üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ `torch_geometric.data.Data`

# %%
num_attr_keys = ['in_deg','out_deg','sent_sum','recv_sum','net_sum','pagerank','clustering']

for n in G.nodes():
    G.nodes[n]['x'] = torch.tensor([float(G.nodes[n][k]) for k in num_attr_keys], dtype=torch.float)

data = from_networkx(G, group_node_attrs=['x'])
data.y = torch.tensor([G.nodes[n].get('label', -1) for n in G.nodes()], dtype=torch.long)

print("Num features:", data.num_node_features)  
print(data)

# %%
import torch

# 1) –°–ø–∏—Å–æ–∫ ¬´—Å—Ç–∞—Ä—ã—Ö¬ª —Ñ–∏—á–µ–π —É–∂–µ –≤ data.x: shape [N,7]
X_base = data.x                # float32, device –º–æ–∂–µ—Ç –±—ã—Ç—å CPU –∏–ª–∏ CUDA

# 2) –î–æ—Å—Ç–∞—ë–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–∫–∞–ª—è—Ä–Ω—ã–µ —Ñ–∏—á–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Ñ–æ—Ä–º–µ [N,1]
btw      = data.btw_centr.view(-1,1)   # [N] ‚Üí [N,1]
wcc      = data.wcc_size.view(-1,1)
send_min = data.send_min.view(-1,1)
recv_min = data.recv_min.view(-1,1)

# 3) Node2Vec: shape [N,128]
X_n2v    = data.n2v               # —É–∂–µ [N,128]

# 4) –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –≤—Å—ë –≤ –æ–¥–∏–Ω [N, 7+4+128 = 139]
data.x = torch.cat([X_base, btw, wcc, send_min, recv_min, X_n2v], dim=1)

print("Now num features:", data.x.size(1))  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 139

# %%
import numpy as np
from sklearn.preprocessing import StandardScaler

# 1) –≤—ã–≥—Ä—É–∂–∞–µ–º data.x –≤ NumPy
X = data.x.cpu().numpy()   # shape [N,139]

# 2) –ª–æ–≥-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
log_idxs = [0,1,2,3,7,8,9,10]
X[:, log_idxs] = np.log1p(X[:, log_idxs])

# 3) —á–∏—Å—Ç–∏–º NaN/Inf ‚Üí –∫–æ–Ω–µ—á–Ω—ã–µ —á–∏—Å–ª–∞
#    nan ‚Üí 0.0, +inf ‚Üí max_float32, -inf ‚Üí min_float32
X = np.nan_to_num(
    X,
    nan=0.0,
    posinf=np.finfo(np.float32).max,
    neginf=np.finfo(np.float32).min
)

# (–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Ç–µ–ø–µ—Ä—å –≤—Å–µ –∫–æ–Ω–µ—á–Ω—ã–µ)
assert np.isfinite(X).all(), "–ï—Å—Ç—å –µ—â—ë –Ω–µ-–∫–æ–Ω–µ—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã!"

# 4) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 5) –æ–±—Ä–∞—Ç–Ω–æ –≤ TorchTensor
import torch
data.x = torch.tensor(X, dtype=torch.float32).to(device)

# %% [markdown]
# ## ‚úÇÔ∏è Train / Val / Test —Å–ø–ª–∏—Ç

# %%
import hashlib
import numpy as np
import torch

# 1) –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–∑–ª–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ G.nodes()
nodes = list(G.nodes())

# 2) –•–µ—à–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π ID (md5 ‚Üí —Ü–µ–ª–æ–µ), –±–µ—Ä—ë–º mod 100 ‚Üí [0..99]
hash_vals = np.array([
    int(hashlib.md5(str(n).encode()).hexdigest(), 16) % 100
    for n in nodes
])

# 3) –ü–æ—Ä–æ–≥: [0,59] ‚Üí train, [60,79] ‚Üí val, [80,99] ‚Üí test
train_mask = torch.tensor(hash_vals < 60, dtype=torch.bool)
val_mask   = torch.tensor((hash_vals >= 60) & (hash_vals < 80), dtype=torch.bool)
test_mask  = torch.tensor(hash_vals >= 80, dtype=torch.bool)

# 4) –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –∫ data
data.train_mask = train_mask
data.val_mask   = val_mask
data.test_mask  = test_mask

# –ü—Ä–æ–≤–µ—Ä–∏–º –¥–æ–ª–∏
print(f"Train: {train_mask.sum().item()/len(nodes):.2%}, "
      f"Val: {val_mask.sum().item()/len(nodes):.2%}, "
      f"Test: {test_mask.sum().item()/len(nodes):.2%}")

train_loader = NeighborLoader(
    data,
    input_nodes=data.train_mask,        # –∫–æ—Ä–Ω–µ–≤—ã–µ —É–∑–ª—ã –¥–ª—è train
    num_neighbors=[25, 15],             # —Å–∫–æ–ª—å–∫–æ —Å–æ—Å–µ–¥–µ–π –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ
    batch_size=1024,
    shuffle=True,
)
val_loader = NeighborLoader(
    data,
    input_nodes=data.val_mask,
    num_neighbors=[25, 15],
    batch_size=1024,
    shuffle=False,
)
test_loader = NeighborLoader(
    data,
    input_nodes=data.test_mask,
    num_neighbors=[25, 15],
    batch_size=1024,
    shuffle=False,
)

# %%
OUT_PATH = Path('./artifacts/eth_graph.pt')
OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
torch.save(data, OUT_PATH)
print(f'–°–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ {OUT_PATH.resolve()}')

# %% [markdown]
# ## üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GNN‚Äë–º–æ–¥–µ–ª–∏ (GCN)

# %%
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv


# 1) –û–ø—Ä–µ–¥–µ–ª—è–µ–º GraphSAGE
class GraphSAGE(torch.nn.Module):
    def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.25):
        super().__init__()
        self.conv1 = SAGEConv(in_ch, hidden_ch)
        self.conv2 = SAGEConv(hidden_ch, out_ch)
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        return x

# %% [markdown]
# ## üèÉ‚Äç‚ôÇÔ∏è –û–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è

# %%
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
data = data.to(device)

labels = data.y[data.y != -1]
counts = torch.bincount(labels)
weight = torch.tensor([1.0, counts[0].float() / counts[1].float()], device=device)
print(f"Class weights: normal={weight[0]:.2f}, fraud={weight[1]:.2f}")

# 3) –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å, optimizer, criterion (—Å –≤–µ—Å–∞–º–∏ –∏–∑ —à–∞–≥–∞ 2)
model = GraphSAGE(
    in_ch=data.num_node_features,
    hidden_ch=256,
    out_ch=int(data.y.max().item())+1,
    dropout=0.25
).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss(weight=weight, ignore_index=-1)

# 4) –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
def train_epoch():
    model.train()
    total_loss = 0
    for batch in train_loader:
        batch = batch.to(device)
        optimizer.zero_grad()
        out = model(batch.x, batch.edge_index)
        # —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ batch.batch_size –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ‚Äî –¥–ª—è –∫–æ—Ä–Ω–µ–≤—ã—Ö —É–∑–ª–æ–≤
        loss = criterion(out[:batch.batch_size], batch.y[:batch.batch_size])
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * batch.batch_size
    return total_loss / int(data.train_mask.sum())

@torch.no_grad()
def eval_loader(loader):
    model.eval()
    correct = total = 0
    for batch in loader:
        batch = batch.to(device)
        out = model(batch.x, batch.edge_index)
        logits = out[:batch.batch_size]
        y = batch.y[:batch.batch_size]

        # –¢–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ
        mask = (y != -1)
        if mask.sum() == 0:
            continue

        preds = logits.argmax(dim=1)
        correct += int((preds[mask] == y[mask]).sum())
        total   += int(mask.sum())

    return correct / total if total > 0 else 0.0

for epoch in range(1, 11):
    loss = train_epoch()
    train_acc = eval_loader(train_loader)
    val_acc   = eval_loader(val_loader)
    print(f'Epoch {epoch:02d} | Loss {loss:.4f} | Train Acc {train_acc:.3f} | Val Acc {val_acc:.3f}')

test_acc = eval_loader(test_loader)
print(f'‚úÖ Test Accuracy: {test_acc:.3f}')

# %% [markdown]
# ## üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# –í—ã—á–∏—Å–ª–∏–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –∏ –æ—Ç–æ–±—Ä–∞–∑–∏–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫.

# %%
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report

model.eval()
all_preds, all_labels = [], []
for batch in test_loader:
    batch = batch.to(device)
    out = model(batch.x, batch.edge_index)
    logits = out[:batch.batch_size]
    y = batch.y[:batch.batch_size]
    mask = (y != -1)

    all_preds.append(logits.argmax(dim=1)[mask].cpu())
    all_labels.append(y[mask].cpu())

y_true = torch.cat(all_labels).numpy()
y_pred = torch.cat(all_preds).numpy()
print(classification_report(y_true, y_pred, digits=3))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d')
plt.title('Confusion Matrix (Test)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# %% [markdown]
# ## üïµÔ∏è‚Äç‚ôÇÔ∏è –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å **GNNExplainer**
# –í—ã–±–µ—Ä–µ–º –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É–∑–µ–ª –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–∏–µ —Ä—ë–±—Ä–∞ –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±—ã–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏.

# %% [markdown]
# ## üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

# %%
MODEL_PATH = Path('./checkpoints/gcn_model.pt')
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
torch.save(model.state_dict(), MODEL_PATH)
print(f'–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH.resolve()}')


