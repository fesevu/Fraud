# %%
%pip uninstall torch torch_geometric torch_scatter torch_sparse pyg_lib torch_cluster torch_spline_conv -y
# 2.1 PyTorch (CPU) ‚Äì –ø–æ–¥–º–µ–Ω–∏—Ç–µ –∏–Ω–¥–µ–∫—Å, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ CUDA
%pip install torch==2.3.0

# 2.2 PyG: —Å 2.3+ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –ø–æ—á—Ç–∏ –Ω–µ—Ç, —Å—Ç–∞–≤–∏–º –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π
%pip install torch_geometric
%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cpu.html


# 2.4 –æ—Å—Ç–∞–ª—å–Ω–æ–µ
%pip install xgboost pandas matplotlib graphviz scikit-learn tqdm numpy networkx seaborn


# %%
from pathlib import Path
import networkx as nx
import pandas as pd
import numpy as np
import torch
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx
from torch_geometric.nn import Node2Vec
from torch_geometric.utils import to_undirected
import math
import torch_cluster

print(torch.__version__)

# %%
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# %% [markdown]
# # –ß—Ç–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞

# %%
from pathlib import Path
ACCOUNTS_CSV = Path('./tmp/Ethereum/account.csv')      # –æ–±–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
TXS_CSV      = Path('./tmp/Ethereum/transaction.csv')   # –æ–±–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

assert ACCOUNTS_CSV.exists(), f'{ACCOUNTS_CSV} not found'
assert TXS_CSV.exists(), f'{TXS_CSV} not found'

# %%
acc_df = pd.read_csv(ACCOUNTS_CSV, header=None, names=['id', 'label'])
tx_df  = pd.read_csv(TXS_CSV, header=None, names=['src', 'dst', 'amount', 'timestamp'])

display(acc_df.head())
display(tx_df.head())

# %% [markdown]
# ## üèóÔ∏è –°–±–æ—Ä–∫–∞ –≥—Ä–∞—Ñ–∞

# %%
G = nx.DiGraph()

# add nodes
for _, row in acc_df.iterrows():
    G.add_node(row.id, label=int(bool(row.label)))

# add edges
for _, row in tx_df.iterrows():
    G.add_edge(row.src, row.dst, amount=float(row.amount), ts=float(row.timestamp))

print(f'–ì—Ä–∞—Ñ —Å–æ–¥–µ—Ä–∂–∏—Ç {G.number_of_nodes():,} —É–∑–ª–æ–≤ –∏ {G.number_of_edges():,} —Ä—ë–±–µ—Ä')

# %%
# –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
in_deg  = dict(G.in_degree())
out_deg = dict(G.out_degree())

# –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å—É–º–º—ã
sent_sum = {n: 0.0 for n in G.nodes()}
recv_sum = {n: 0.0 for n in G.nodes()}
for u, v, d in G.edges(data=True):
    amt = d['amount']
    sent_sum[u] += amt
    recv_sum[v] += amt

# PageRank –∏ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
pr = nx.pagerank(G, alpha=0.85)
clust = nx.clustering(G.to_undirected())

# –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∏—á–∏ –≤ –≥—Ä–∞—Ñ
for n in G.nodes():
    G.nodes[n].update({
        'in_deg':      in_deg.get(n, 0),
        'out_deg':     out_deg.get(n, 0),
        'sent_sum':    sent_sum[n],
        'recv_sum':    recv_sum[n],
        'net_sum':     sent_sum[n] - recv_sum[n],
        'pagerank':    pr[n],
        'clustering':  clust[n],
    })

# --- –ø–æ—Å–ª–µ —Ä–∞—Å—á—ë—Ç–∞ pr –∏ clust --------------------------------------------
print('‚è≥ Calculating extra graph features‚Ä¶')

# 1) Betweenness (approx)
btw = nx.betweenness_centrality(G, k=10_000, seed=42, normalized=True)

# 2) Weakly-connected component size
G_u = G.to_undirected()
wcc = {n: 0 for n in G}
for comp in nx.connected_components(G_u):
    size = len(comp)
    for n in comp:
        wcc[n] = size

# 3) Min amounts
send_min = {n: math.inf for n in G}
recv_min = {n: math.inf for n in G}
for u, v, d in G.edges(data=True):
    amt = d['amount']
    send_min[u] = min(send_min[u], amt)
    recv_min[v] = min(recv_min[v], amt)
# –∑–∞–º–µ–Ω—è–µ–º inf ‚Üí 0.0 (—É–∑–ª—ã –±–µ–∑ –æ–ø–µ—Ä–∞—Ü–∏–π)
send_min = {n: 0.0 if math.isinf(v) else v for n, v in send_min.items()}
recv_min = {n: 0.0 if math.isinf(v) else v for n, v in recv_min.items()}

# --- –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Å–µ –Ω–æ–≤—ã–µ —Ñ–∏—á–∏ –≤ –≤–µ—Ä—à–∏–Ω—ã ----------------------------------
for n in G.nodes():
    G.nodes[n].update({
        'btw_centr':   btw[n],
        'wcc_size':    wcc[n],
        'send_min':    send_min[n],
        'recv_min':    recv_min[n],
    })

# %%
 # --- –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º Node2Vec: —Å–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ id ‚Üí idx -------------------
id2idx = {n: i for i, n in enumerate(G.nodes())}

# --- —Å—Ç—Ä–æ–∏–º edge_index –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∏–Ω–¥–µ–∫—Å–∞–º ------------------------------
edges = list(G.edges())
row = [id2idx[u] for u, v in edges]
col = [id2idx[v] for u, v in edges]
edge_index = torch.tensor([row, col], dtype=torch.long)
edge_index = to_undirected(edge_index).contiguous()

# --- —Ç–µ–ø–µ—Ä—å –æ–±—É—á–∞–µ–º Node2Vec –Ω–∞ –Ω–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≥—Ä–∞—Ñ–µ --------------------
print('‚è≥ Training Node2Vec‚Ä¶')
n2v = Node2Vec(
    edge_index=edge_index,
    num_nodes=G.number_of_nodes(),
    embedding_dim=128,
    walk_length=20,
    context_size=10,
    walks_per_node=5,
    num_negative_samples=1,
    sparse=True,
    p=1, q=1
).to(device)

loader = n2v.loader(batch_size=1024, shuffle=True)
opt = torch.optim.SparseAdam(list(n2v.parameters()), lr=0.01)

n2v.train()
for epoch in range(25):  # –º–æ–∂–µ—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å —á–∏—Å–ª–æ —ç–ø–æ—Ö
    total_loss = 0
    for pos_rw, neg_rw in loader:
        opt.zero_grad()
        loss = n2v.loss(pos_rw.to(device), neg_rw.to(device))
        loss.backward()
        opt.step()
        total_loss += loss.item()
    print(f'Epoch {epoch+1}, Loss {total_loss:.4f}')

# %%
# --- –∏–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ G ------------------------
z = n2v.embedding.weight.detach().cpu()
for n, idx in id2idx.items():
    G.nodes[n]['n2v'] = z[idx]

# --- –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ data.x ---------------------------------------------
# –ù–∞–∫–æ–Ω–µ—Ü, —Å–æ–±–∏—Ä–∞–µ–º x
num_attr_keys = [
    'in_deg','out_deg','sent_sum','recv_sum','net_sum',
    'pagerank','clustering',
    'btw_centr','wcc_size','send_min','recv_min'
]

for n in G.nodes():
    # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å—ë –µ—Å—Ç—å
    missing = [k for k in num_attr_keys if k not in G.nodes[n]]
    if missing:
        raise RuntimeError(f"–£ —É–∑–ª–∞ {n} –Ω–µ—Ç —Ñ–∏—á–µ–π {missing}")
    base = [float(G.nodes[n][k]) for k in num_attr_keys]
    emb  = G.nodes[n]['n2v'].tolist()           # 128-–º–µ—Ä–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
    G.nodes[n]['x'] = torch.tensor(base + emb, dtype=torch.float)

print('–ü—Ä–∏–º–µ—Ä —Ñ–∏—á —É–∑–ª–∞:', list(G.nodes(data=True))[0])

# %%
# üîß –î–µ–ª–∞–µ–º —Ç–∞–∫, —á—Ç–æ–±—ã —É –≤—Å–µ—Ö —É–∑–ª–æ–≤ –±—ã–ª–∞ –º–µ—Ç–∫–∞
nx.set_node_attributes(G, -1, "label")        # —Å–Ω–∞—á–∞–ª–∞ –≤—Å–µ–º —Å—Ç–∞–≤–∏–º -1
for _, row in acc_df.iterrows():              # –∑–∞—Ç–µ–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º, —á—Ç–æ –µ—Å—Ç—å –≤ accounts.csv
    G.nodes[row.id]["label"] = int(bool(row.label))   

# %%
# –ø–æ—Å–ª–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –¥–æ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ PyG
from sklearn.preprocessing import StandardScaler
X = np.stack([G.nodes[n]['x'].numpy() for n in G.nodes()])
scaler = StandardScaler()
X = scaler.fit_transform(X)
for n, vec in zip(G.nodes(), X):
    G.nodes[n]['x'] = torch.tensor(vec, dtype=torch.float)

# %% [markdown]
# ## üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ `torch_geometric.data.Data`

# %%
num_attr_keys = ['in_deg','out_deg','sent_sum','recv_sum','net_sum','pagerank','clustering']

for n in G.nodes():
    G.nodes[n]['x'] = torch.tensor([float(G.nodes[n][k]) for k in num_attr_keys], dtype=torch.float)

data = from_networkx(G, group_node_attrs=['x'])
data.y = torch.tensor([G.nodes[n].get('label', -1) for n in G.nodes()], dtype=torch.long)

print("Num features:", data.num_node_features)  
print(data)

# %%
import torch

# 1) –°–ø–∏—Å–æ–∫ ¬´—Å—Ç–∞—Ä—ã—Ö¬ª —Ñ–∏—á–µ–π —É–∂–µ –≤ data.x: shape [N,7]
X_base = data.x                # float32, device –º–æ–∂–µ—Ç –±—ã—Ç—å CPU –∏–ª–∏ CUDA

# 2) –î–æ—Å—Ç–∞—ë–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–∫–∞–ª—è—Ä–Ω—ã–µ —Ñ–∏—á–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Ñ–æ—Ä–º–µ [N,1]
btw      = data.btw_centr.view(-1,1)   # [N] ‚Üí [N,1]
wcc      = data.wcc_size.view(-1,1)
send_min = data.send_min.view(-1,1)
recv_min = data.recv_min.view(-1,1)

# 3) Node2Vec: shape [N,128]
X_n2v    = data.n2v               # —É–∂–µ [N,128]

# 4) –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –≤—Å—ë –≤ –æ–¥–∏–Ω [N, 7+4+128 = 139]
data.x = torch.cat([X_base, btw, wcc, send_min, recv_min, X_n2v], dim=1)

print("Now num features:", data.x.size(1))  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 139

# %%
import numpy as np
from sklearn.preprocessing import StandardScaler

# 1) –≤—ã–≥—Ä—É–∂–∞–µ–º data.x –≤ NumPy
X = data.x.cpu().numpy()   # shape [N,139]

# 2) –ª–æ–≥-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
log_idxs = [0,1,2,3,7,8,9,10]
X[:, log_idxs] = np.log1p(X[:, log_idxs])

# 3) —á–∏—Å—Ç–∏–º NaN/Inf ‚Üí –∫–æ–Ω–µ—á–Ω—ã–µ —á–∏—Å–ª–∞
#    nan ‚Üí 0.0, +inf ‚Üí max_float32, -inf ‚Üí min_float32
X = np.nan_to_num(
    X,
    nan=0.0,
    posinf=np.finfo(np.float32).max,
    neginf=np.finfo(np.float32).min
)

# (–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Ç–µ–ø–µ—Ä—å –≤—Å–µ –∫–æ–Ω–µ—á–Ω—ã–µ)
assert np.isfinite(X).all(), "–ï—Å—Ç—å –µ—â—ë –Ω–µ-–∫–æ–Ω–µ—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã!"

# 4) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 5) –æ–±—Ä–∞—Ç–Ω–æ –≤ TorchTensor
import torch
data.x = torch.tensor(X, dtype=torch.float32).to(device)

# %% [markdown]
# ## ‚úÇÔ∏è Train / Val / Test —Å–ø–ª–∏—Ç

# %%
import hashlib
import numpy as np
import torch

# 1) –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–∑–ª–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ G.nodes()
nodes = list(G.nodes())

# 2) –•–µ—à–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π ID (md5 ‚Üí —Ü–µ–ª–æ–µ), –±–µ—Ä—ë–º mod 100 ‚Üí [0..99]
hash_vals = np.array([
    int(hashlib.md5(str(n).encode()).hexdigest(), 16) % 100
    for n in nodes
])

# 3) –ü–æ—Ä–æ–≥: [0,59] ‚Üí train, [60,79] ‚Üí val, [80,99] ‚Üí test
train_mask = torch.tensor(hash_vals < 60, dtype=torch.bool)
val_mask   = torch.tensor((hash_vals >= 60) & (hash_vals < 80), dtype=torch.bool)
test_mask  = torch.tensor(hash_vals >= 80, dtype=torch.bool)

# –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è data.y (—Å -1 –¥–ª—è –Ω–µ–º–µ—Ç–æ—á–Ω—ã—Ö) –∏ –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è loader‚Äô–æ–≤
label_mask = data.y != -1               # –±—É–ª–µ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä: True —Ç–∞–º, –≥–¥–µ –µ—Å—Ç—å 0/1
train_mask = (hash_vals < 60)  & label_mask.cpu().numpy()
val_mask   = ((hash_vals >= 60) & (hash_vals < 80)) & label_mask.cpu().numpy()
test_mask  = (hash_vals >= 80) & label_mask.cpu().numpy()

# –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ torch.bool
data.train_mask = torch.tensor(train_mask, dtype=torch.bool, device=device)
data.val_mask   = torch.tensor(val_mask,   dtype=torch.bool, device=device)
data.test_mask  = torch.tensor(test_mask,  dtype=torch.bool, device=device)

# –ü—Ä–æ–≤–µ—Ä–∏–º –¥–æ–ª–∏
print(f"Train: {train_mask.sum().item()/len(nodes):.2%}, "
      f"Val: {val_mask.sum().item()/len(nodes):.2%}, "
      f"Test: {test_mask.sum().item()/len(nodes):.2%}")

train_loader = NeighborLoader(
    data,
    input_nodes=data.train_mask,        # –∫–æ—Ä–Ω–µ–≤—ã–µ —É–∑–ª—ã –¥–ª—è train
    num_neighbors=[200, 200],             # —Å–∫–æ–ª—å–∫–æ —Å–æ—Å–µ–¥–µ–π –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ
    batch_size=1024,
    shuffle=True,
)
val_loader = NeighborLoader(
    data,
    input_nodes=data.val_mask,
    num_neighbors=[200, 200],
    batch_size=1024,
    shuffle=False,
)
test_loader = NeighborLoader(
    data,
    input_nodes=data.test_mask,
    num_neighbors=[200, 200],
    batch_size=1024,
    shuffle=False,
)


# %%
OUT_PATH = Path('./artifacts/eth_graph.pt')
OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
torch.save(data, OUT_PATH)
print(f'–°–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ {OUT_PATH.resolve()}')

# %% [markdown]
# ## üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GNN‚Äë–º–æ–¥–µ–ª–∏ (GCN)

# %%
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv


# 1) –û–ø—Ä–µ–¥–µ–ª—è–µ–º GraphSAGE
class GraphSAGE(torch.nn.Module):
    def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.25):
        super().__init__()
        self.conv1 = SAGEConv(in_ch, hidden_ch)
        self.conv2 = SAGEConv(hidden_ch, out_ch)
        self.dropout1 = torch.nn.Dropout(dropout)
        self.dropout2 = torch.nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout1(x)
        x = self.conv2(x, edge_index)
        x = self.dropout2(x)
        return x

# %% [markdown]
# ## üèÉ‚Äç‚ôÇÔ∏è –û–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è

# %%
from torch.optim.lr_scheduler import ReduceLROnPlateau
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
data = data.to(device)

labels = data.y[data.y != -1]
counts = torch.bincount(labels)
weight = torch.tensor([1.0, counts[0].float() / counts[1].float()], device=device)
print(f"Class weights: normal={weight[0]:.2f}, fraud={weight[1]:.2f}")

# 3) –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å, optimizer, criterion (—Å –≤–µ—Å–∞–º–∏ –∏–∑ —à–∞–≥–∞ 2)
model = GraphSAGE(
    in_ch=data.num_node_features,
    hidden_ch=256,
    out_ch=int(data.y.max().item())+1,
    dropout=0.45
).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)
criterion = torch.nn.CrossEntropyLoss(weight=weight)

# 4) –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
def train_epoch():
    model.train()
    total_loss = 0
    for batch in train_loader:
        batch = batch.to(device)
        optimizer.zero_grad()
        out = model(batch.x, batch.edge_index)
        y = batch.y[:batch.batch_size]
        mask = y != -1                    # True –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö –º–µ—Ç–æ–∫
        if mask.sum() == 0:
            continue
        # —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ batch.batch_size –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ‚Äî –¥–ª—è –∫–æ—Ä–Ω–µ–≤—ã—Ö —É–∑–ª–æ–≤
        loss = criterion(out[:batch.batch_size], batch.y[:batch.batch_size])
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * batch.batch_size
    return total_loss / int(data.train_mask.sum())

@torch.no_grad()
def eval_loader(loader):
    model.eval()
    correct = total = 0
    for batch in loader:
        batch = batch.to(device)
        out = model(batch.x, batch.edge_index)
        logits = out[:batch.batch_size]
        y = batch.y[:batch.batch_size]

        # –¢–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ
        mask = (y != -1)
        if mask.sum() == 0:
            continue

        preds = logits.argmax(dim=1)
        correct += int((preds[mask] == y[mask]).sum())
        total   += int(mask.sum())

    return correct / total if total > 0 else 0.0

num_epochs = 30
train_losses, val_losses = [], []
train_accs, val_accs     = [], []

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–π –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
@torch.no_grad()
def eval_loss(loader):
    model.eval()
    total_loss, total = 0.0, 0
    for batch in loader:
        batch = batch.to(device)
        out = model(batch.x, batch.edge_index)
        logits = out[:batch.batch_size]
        y = batch.y[:batch.batch_size]
        mask = (y != -1)
        if mask.sum() == 0:
            continue
        loss = criterion(logits[mask], y[mask])
        total_loss += loss.item() * mask.sum().item()
        total += mask.sum().item()
    return total_loss / total if total > 0 else 0.0

scheduler = ReduceLROnPlateau(optimizer,
                              mode='min',
                              factor=0.5,
                              patience=3,
                              verbose=True)

best_val_loss = float('inf')
patience_cnt = 0
early_stop_patience = 7

for epoch in range(1, num_epochs+1):
    loss = train_epoch()
    train_acc = eval_loader(train_loader)
    val_loss  = eval_loss(val_loader)
    val_acc   = eval_loader(val_loader)

    train_losses.append(loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

    # –®–µ–¥—É–ª–µ—Ä –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–µ
    scheduler.step(val_loss)

    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pt')
        patience_cnt = 0
    else:
        patience_cnt += 1
        if patience_cnt >= early_stop_patience:
            print(f"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
            break

    print(f'Epoch {epoch:02d} | Train Loss {loss:.4f} | Val Loss {val_loss:.4f} '
          f'| Train Acc {train_acc:.3f} | Val Acc {val_acc:.3f}')


test_acc = eval_loader(test_loader)
print(f'‚úÖ Test Accuracy: {test_acc:.3f}')

# %%

# ‚îÄ‚îÄ‚îÄ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import matplotlib.pyplot as plt

epochs = list(range(1, len(train_losses) + 1))  # –≤–º–µ—Å—Ç–æ num_epochs
plt.figure(figsize=(8,4))
plt.plot(epochs, train_losses, label='Train Loss')
plt.plot(epochs, val_losses,   label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Train vs Val Loss')
plt.show()

plt.figure(figsize=(8,4))
plt.plot(epochs, train_accs, label='Train Accuracy')
plt.plot(epochs, val_accs,   label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

# %% [markdown]
# ## üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# –í—ã—á–∏—Å–ª–∏–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –∏ –æ—Ç–æ–±—Ä–∞–∑–∏–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫.

# %%
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report

model.eval()
all_preds, all_labels = [], []
for batch in test_loader:
    batch = batch.to(device)
    out = model(batch.x, batch.edge_index)
    logits = out[:batch.batch_size]
    y = batch.y[:batch.batch_size]
    mask = (y != -1)

    all_preds.append(logits.argmax(dim=1)[mask].cpu())
    all_labels.append(y[mask].cpu())

y_true = torch.cat(all_labels).numpy()
y_pred = torch.cat(all_preds).numpy()
print(classification_report(y_true, y_pred, digits=3))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d')
plt.title('Confusion Matrix (Test)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# %% [markdown]
# ## üïµÔ∏è‚Äç‚ôÇÔ∏è –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å **GNNExplainer**
# –í—ã–±–µ—Ä–µ–º –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É–∑–µ–ª –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–∏–µ —Ä—ë–±—Ä–∞ –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±—ã–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏.

# %%
# %% 
# ## üïµÔ∏è‚Äç‚ôÇÔ∏è –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å GNNExplainer (inline, —Å –∏–º–µ–Ω–∞–º–∏ —Ñ–∏—á–µ–π)

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from torch_geometric.explain import Explainer, GNNExplainer

# 1) –ù–∞–π—Ç–∏ –ø–µ—Ä–≤—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É–∑–µ–ª
model.eval()
target_node = None
for batch in test_loader:
    batch = batch.to(device)
    out   = model(batch.x, batch.edge_index)
    preds = out[:batch.batch_size].argmax(dim=1)
    y     = batch.y[:batch.batch_size]
    mask  = (y != -1) & (preds == y)
    if mask.any():
        idx_local   = mask.nonzero(as_tuple=True)[0][0].item()
        target_node = batch.n_id[idx_local].item()
        break

print(f'üîç –û–±—ä—è—Å–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —É–∑–ª–∞ {target_node}')

# 2) –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å Explainer
explainer = Explainer(
    model=model,
    algorithm=GNNExplainer(epochs=200),
    explanation_type='model',
    node_mask_type='attributes',
    edge_mask_type='object',
    model_config=dict(
        mode='multiclass_classification',
        task_level='node',
        return_type='raw',
    ),
)

# 3) –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å Explanation
explanation = explainer(
    x=data.x, 
    edge_index=data.edge_index, 
    index=target_node,
)

# 4) –ë–∞—Ä-–≥—Ä–∞—Ñ —Ç–æ–ø-10 —Ñ–∏—á —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
feat_names = [
    'in_deg','out_deg','sent_sum','recv_sum','net_sum',
    'pagerank','clustering',
    'btw_centr','wcc_size','send_min','recv_min'
] + [f'n2v_{i}' for i in range(128)]

# —Ä–∞—Å–ø–ª—é—â–∏–≤–∞–µ–º –≤ 1D-–º–∞—Å—Å–∏–≤
feat_imp = explanation.node_mask.cpu().numpy().flatten()
top_k    = 10

# —Ç–µ–ø–µ—Ä—å argsort –≤–µ—Ä–Ω—ë—Ç –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤
idxs = np.argsort(feat_imp)[-top_k:][::-1].tolist()
vals = feat_imp[idxs]
names = [feat_names[i] for i in idxs]

plt.figure(figsize=(8,4))
plt.barh(names, vals)
plt.xlabel('Importance')
plt.title(f'Top {top_k} Feature Importances for node {target_node}')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# 5) –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä—è—Å–Ω—è—é—â–µ–≥–æ —Å—É–±–≥—Ä–∞—Ñ–∞ –∫–∞–∫ –Ω–∞ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

# –î–æ—Å—Ç–∞—ë–º directed edge-mask –∏ edge_index
edge_mask = explanation.edge_mask.cpu().numpy().flatten()
E = data.edge_index.cpu().numpy()
mask = edge_mask > np.percentile(edge_mask, 100 - 20)  # —Ç–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Ä—ë–±–µ—Ä

# –§–æ—Ä–º–∏—Ä—É–µ–º DiGraph –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —Ä—ë–±–µ—Ä
sub_edges = list(zip(E[0][mask], E[1][mask]))
G_sub = nx.DiGraph()
G_sub.add_edges_from(sub_edges)

# –î–æ—Å—Ç–∞—ë–º layout
pos = nx.spring_layout(G_sub, seed=42)

# –†–∏—Å—É–µ–º –≤—Å—ë inline
fig, ax = plt.subplots(figsize=(6,6))
# —Ä–∞–º–∫–∞
ax.add_patch(Rectangle(
    ( -1.05, -1.05),   # —á—É—Ç—å-—á—É—Ç—å –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã layout [-1..1]
    2.1, 2.1,
    fill=False, lw=1.5, edgecolor='black'
))

# —É–∑–ª—ã
nx.draw_networkx_nodes(
    G_sub, pos,
    ax=ax,
    node_size=800,
    node_color='white',
    edgecolors='black',
    linewidths=1.2
)

# —Å—Ç—Ä–µ–ª–∫–∏
nx.draw_networkx_edges(
    G_sub, pos,
    ax=ax,
    arrowstyle='-|>',
    arrowsize=12,
    width=1.2,
    edge_color='black',
    connectionstyle='arc3,rad=0.1'
)

# –ø–æ–¥–ø–∏—Å–∏ –≤–Ω—É—Ç—Ä–∏ –∫—Ä—É–∂–∫–æ–≤
nx.draw_networkx_labels(
    G_sub, pos,
    ax=ax,
    labels={n:str(n) for n in G_sub.nodes()},
    font_size=10
)

ax.set_xticks([])
ax.set_yticks([])
ax.set_xlim(-1.1, 1.1)
ax.set_ylim(-1.1, 1.1)
ax.axis('off')
plt.title(f'Explanation Subgraph for node {target_node}', pad=15)
plt.tight_layout()
plt.show()

# %% [markdown]
# ## üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

# %%
MODEL_PATH = Path('./checkpoints/gcn_model.pt')
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
torch.save(model.state_dict(), MODEL_PATH)
print(f'–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH.resolve()}')

# %%
import torch
import pickle
import numpy as np
from pathlib import Path

# 1) –ü–∞–ø–∫–∞ –¥–ª—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
artifacts_dir = Path("./artifacts")
artifacts_dir.mkdir(exist_ok=True)

# 2) –í–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–∏ early stopping) —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–µ –≤–µ—Åa
best_val_loss = float('inf')
for epoch in range(1, num_epochs+1):
    train_epoch()
    val_loss = eval_loss(val_loader)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(
            model.state_dict(),
            artifacts_dir / "gcn_best_weights.pt"
        )
        print(f"[Epoch {epoch}] –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ –ª—É—á—à–∏–µ –≤–µ—Å–∞ (val_loss={val_loss:.4f})")

# 3) –ü–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ–º:
# 3.1) —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
torch.save(
    model.state_dict(),
    artifacts_dir / "gcn_final_weights.pt"
)
print("–§–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ artifacts/gcn_final_weights.pt")

# 3.2) –≤–µ—Å—å –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É–¥–æ–±–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–µ–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞)
torch.save(
    model,
    artifacts_dir / "gcn_model_full.pt"
)
print("–ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ artifacts/gcn_model_full.pt")

# 3.3) scaler (StandardScaler –∏–∑ sklearn)
with open(artifacts_dir / "scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)
print("Scaler —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ artifacts/scaler.pkl")

# 3.4) —Å–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –≤—ã —Å–æ–±–∏—Ä–∞–ª–∏ –≤ x
#      –≤–æ–∑—å–º–∏—Ç–µ –≤–∞—à DATA_FEATS —Å–ø–∏—Å–æ–∫ (–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ data.num_node_features –Ω–∞–∑–≤–∞–Ω–∏–π)
feature_list = feat_names  # –Ω–∞–ø—Ä–∏–º–µ—Ä ['in_degree','out_degree',‚Ä¶,'wcc_size']
with open(artifacts_dir / "features_list.pkl", "wb") as f:
    pickle.dump(feature_list, f)
print("–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ artifacts/features_list.pkl")

# 4) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (Node2Vec –∏–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö x-–≤–µ–∫—Ç–æ—Ä–æ–≤)
#    ‚Äî –ª–∏–±–æ –±–µ—Ä—ë–º z = n2v.embedding.weight.detach().cpu().numpy()
#    ‚Äî –ª–∏–±–æ –≤–∞—à–∏ –∏—Ç–æ–≥–æ–≤—ã–µ node representations data.x.cpu().numpy()
embeddings = n2v.embedding.weight.detach().cpu().numpy()
np.save(artifacts_dir / "node2vec_embeddings.npy", embeddings)
print("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ artifacts/node2vec_embeddings.npy")

# %% [markdown]
# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

# %%
# 1) –ú–æ–¥–µ–ª—å –∏ –≤–µ—Åa
model = GraphSAGE(...)    # –æ–±—ä—è–≤–∏—Ç—å —Ç—É –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
model.load_state_dict(torch.load("./artifacts/gcn_best_weights.pt"))
model.eval()

# –∏–ª–∏, –µ—Å–ª–∏ –≤—ã —Å–æ—Ö—Ä–∞–Ω—è–ª–∏ –≤–µ—Å—å –æ–±—ä–µ–∫—Ç:
model = torch.load("./artifacts/gcn_model_full.pt")
model.eval()

# 2) Scaler
with open("./artifacts/scaler.pkl","rb") as f:
    scaler = pickle.load(f)

# 3) –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π
with open("./artifacts/features_list.pkl","rb") as f:
    feature_list = pickle.load(f)

# 4) –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
embeddings = np.load("./artifacts/node2vec_embeddings.npy")


