# %%
# !pip uninstall -y flax orbax-checkpoint jax jaxlib \
#     ml-dtypes tf-keras tensorflow tensorflow-cpu tensorflow-text \
#     tensorflow-decision-forests keras keras-hub \
#     chex optax fastai spacy tensorstore numba \
#     umap-learn pynndescent librosa shap cuml-cu12 cudf-cu12 dask-cuda

import tensorflow as tf
import seaborn as sns
import random
import importlib
from pathlib import Path
import pickle
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.models import Sequential
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import numpy as np
from kaggle.api.kaggle_api_extended import KaggleApi
import zipfile
import pandas as pd
import pathlib
import json
import os
%pip install - U threadpoolctl joblib
# %pip install --force-reinstall numpy==1.26.4
%pip install - -upgrade scikit-learn
%pip install - U imbalanced-learn

%pip install - -upgrade - -force-reinstall \
    numpy \
    scipy \
    matplotlib \
    seaborn \
    pandas \
    tensorflow

# %%

print("===== VERSIONS =====")
for lib in ("numpy", "pandas", "sklearn", "imblearn",
            "tensorflow", "matplotlib", "seaborn"):
    m = importlib.import_module(lib if lib != "sklearn" else "sklearn")
    print(f"{lib:17s}: {m.__version__}")

print("\n===== TensorFlow devices =====")
# ‚Üê –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Ç–æ–ª—å–∫–æ CPU, –∏ —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
print(tf.config.list_physical_devices())

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.keras.utils.set_random_seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)
print(f"\nRandom seed set to {SEED}")

# %%
# === @title –≠—Ç–∞–ø 2 (Kaggle API) ‚Äî Ethereum Fraud Detection ===
# 0. (–µ—Å–ª–∏ –¥–µ–ª–∞–ª–∏ —Ä–∞–Ω—å—à–µ, —à–∞–≥–∏ —Å pip –∏ —Ç–æ–∫–µ–Ω–æ–º –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å)

# ‚Äî 0.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ kaggle ‚Äî
%pip install - -quiet kaggle

# # ‚Äî 0.2 –ó–∞–≥—Ä—É–∑–∫–∞ kaggle.json (–æ–¥–∏–Ω —Ä–∞–∑ –∑–∞ —Å–µ—Å—Å–∏—é Colab) ‚Äî
# #    –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω —É–∂–µ –ª–µ–∂–∏—Ç –≤ ~/.kaggle, —ç—Ç—É —Å—Ç—Ä–æ–∫—É –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å.
# from google.colab import files, auth
# import pathlib, io, os, json, pandas as pd

# if not pathlib.Path("~/.kaggle/kaggle.json").expanduser().exists():
#     print("üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç–µ kaggle.json ‚ûú")
#     token_file = files.upload()           # –æ—Ç–∫—Ä–æ–µ—Ç—Å—è –¥–∏–∞–ª–æ–≥
#     if "kaggle.json" not in token_file:
#         raise ValueError("–ù—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –ø–æ–¥ –∏–º–µ–Ω–µ–º kaggle.json")
#     pathlib.Path("~/.kaggle").expanduser().mkdir(exist_ok=True)
#     with open(pathlib.Path("~/.kaggle/kaggle.json").expanduser(), "wb") as f:
#         f.write(token_file["kaggle.json"])
#     !chmod 600 ~/.kaggle/kaggle.json
# else:
#     print("üîë kaggle.json —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")


# 1. –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å —Ñ–∞–π–ª kaggle.json (—Å —Ç–æ–∫–µ–Ω–æ–º Kaggle)
kaggle_json_path = '../kaggle.json'

if not os.path.exists(kaggle_json_path):
    raise FileNotFoundError(
        "–§–∞–π–ª 'kaggle.json' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–∞—á–∞–π—Ç–µ –µ–≥–æ —Å –≤–∞—à–µ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ Kaggle.")

# 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º API Kaggle —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–∞
with open(kaggle_json_path) as f:
    kaggle_json = json.load(f)

os.environ['KAGGLE_USERNAME'] = kaggle_json['username']
os.environ['KAGGLE_KEY'] = kaggle_json['key']

# 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º API
api = KaggleApi()
api.authenticate()

# 1. –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞
slug = "vagifa/ethereum-frauddetection-dataset"
print(f"\n‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞—é –¥–∞—Ç–∞—Å–µ—Ç {slug} ...")
!kaggle datasets download - d {slug} - p ./data - -force - -quiet

# 2. –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
print("üì¶ –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞—é ...")
!unzip - o ./data/*.zip - d ./data > /dev/null

# 3. –û—Ç–∫—Ä—ã–≤–∞–µ–º transactions.csv
csv_path = "./data/transactions.csv"
if not os.path.exists(csv_path):
    # fallback: –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π CSV
    import glob
    found = glob.glob("./data/**/*.csv", recursive=True)
    if not found:
        raise RuntimeError("CSV-—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –∞—Ä—Ö–∏–≤–µ.")
    csv_path = found[0]
print(f"‚úÖ –ù–∞—à—ë–ª CSV: {csv_path}")

df = pd.read_csv(csv_path)

# 4. –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
print("\n===== SHAPE =====")
print(df.shape)            # –æ–∂–∏–¥–∞–µ–º (9840, 49)

print("\n===== HEAD (5 —Å—Ç—Ä–æ–∫) =====")
display(df.head())

print("\n===== –ö–ª–∞—Å—Å–æ–≤–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ (isFraud) =====")
if "FLAG" in df.columns:
    display((df["FLAG"].value_counts(normalize=True)
            * 100).round(2).rename("%"))
else:
    print("–°—Ç–æ–ª–±–µ—Ü 'FLAG' –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞.")

# %%
df.drop(columns=["Unnamed: 0", "Index"], inplace=True, errors="ignore")
df['FLAG'].value_counts(normalize=True)

# %%
# === @title –≠—Ç–∞–ø 3: –ø–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===

# 0) –ö–æ–ø–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π df (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
df_eda = df.copy()

# 1) –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
svc_cols = ["Unnamed: 0", "Index"]
df_eda.drop(columns=[c for c in svc_cols if c in df_eda.columns],
            inplace=True, errors="ignore")

print("==> Shape –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Å–ª—É–∂–µ–±–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫:", df_eda.shape)

# 2) –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
na_counts = df_eda.isna().sum()
na_nonzero = na_counts[na_counts > 0].sort_values(ascending=False)

print("\n===== –¢–û–ü-10 —Å—Ç–æ–ª–±—Ü–æ–≤ –ø–æ —á–∏—Å–ª—É NaN =====")
display(na_nonzero.head(10))

# 3) –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –ü–∏—Ä—Å–æ–Ω–∞
num_cols = df_eda.select_dtypes(include=[np.number]).columns.tolist()
corr = df_eda[num_cols].corr(method="pearson")

plt.figure(figsize=(10, 8))
sns.heatmap(corr, cmap="coolwarm", center=0,
            vmax=1, vmin=-1, square=True,
            cbar_kws={"shrink": .8}, xticklabels=False, yticklabels=False)
plt.title("Correlation heatmap (numeric features)")
plt.show()

# 4) –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π (–ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
zero_var = df_eda[num_cols].loc[:,
                                df_eda[num_cols].nunique() == 1].columns.tolist()
print(f"\n===== –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π (n={len(zero_var)}) =====")
print(zero_var)

# 5) –ë—ã—Å—Ç—Ä—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (FLAG)
print("\n===== –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ FLAG =====")
display(df_eda["FLAG"].value_counts(
    normalize=True).rename("%").mul(100).round(2))

# %%
# === @title –≠—Ç–∞–ø 4: –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º ===


# 0) –ö–æ–ø–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π df
df_clean = df.copy()

# 1) –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
df_clean.columns = df_clean.columns.str.strip()
df_clean.drop(columns=["Unnamed: 0", "Index"], inplace=True, errors="ignore")

# 2) –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã (Address, —Ç–æ–∫–µ–Ω—ã –∏ —Ç.–ø.)
str_cols = df_clean.select_dtypes(include=["object"]).columns.tolist()
print("–£–¥–∞–ª—è—é —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏:", str_cols)
df_clean = df_clean.drop(columns=str_cols)

# 3) –£–¥–∞–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
zero_var = [
    "ERC20 avg time between sent tnx",
    "ERC20 avg time between rec tnx",
    "ERC20 avg time between rec 2 tnx",
    "ERC20 avg time between contract tnx",
    "ERC20 min val sent contract",
    "ERC20 max val sent contract",
    "ERC20 avg val sent contract",
]
to_drop = [c for c in zero_var if c in df_clean.columns]
print("–£–¥–∞–ª—è—é zero-variance:", to_drop)
df_clean = df_clean.drop(columns=to_drop)

# 4) –ó–∞–ø–æ–ª–Ω—è–µ–º –≤—Å–µ NaN –º–µ–¥–∏–∞–Ω–æ–π –ø–æ —Å—Ç–æ–ª–±—Ü—É
num_cols = df_clean.columns.tolist()  # —Ç–µ–ø–µ—Ä—å –≤—Å–µ ‚Äî —á–∏—Å–ª–æ–≤—ã–µ
for col in num_cols:
    df_clean[col] = df_clean[col].fillna(df_clean[col].median())

# 5) –£–±–∏—Ä–∞–µ–º –æ–¥–Ω—É –∏–∑ –ø–∞—Ä—ã —Å–∏–ª—å–Ω–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (|œÅ| > 0.9)
corr = df_clean.corr().abs()
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
high_corr_drop = [col for col in upper.columns if any(upper[col] > 0.9)]
print("–£–¥–∞–ª—è—é —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ (>0.9):", high_corr_drop)
df_clean = df_clean.drop(columns=high_corr_drop)

# 6) –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
print("\n–ò—Ç–æ–≥–æ–≤–∞—è —Ñ–æ—Ä–º–∞ df_clean:", df_clean.shape)
print("–û—Å—Ç–∞–ª–æ—Å—å NaN –≤—Å–µ–≥–æ:", df_clean.isna().sum().sum())

# %%
# === @title –≠—Ç–∞–ø 5: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏ SMOTE ===

# 1) –ì–æ—Ç–æ–≤–∏–º X –∏ y
SEED = 42
X = df_clean.drop(columns=["FLAG"])
y = df_clean["FLAG"]

# 2) –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=SEED
)

# 3) –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ SMOTE
print(">> –î–æ SMOTE (train):")
print(y_train.value_counts(), "\n")

# 4) –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE
smote = SMOTE(random_state=SEED)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# 5) –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ SMOTE
print(">> –ü–æ—Å–ª–µ SMOTE (train_resampled):")
print(y_train_res.value_counts())

# %%
# === @title –≠—Ç–∞–ø 6 (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π): –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –≤—Å—ë–º –¥–∞—Ç–∞—Å–µ—Ç–µ ===

# 1) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞: –±–µ—Ä—ë–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫—Ä–æ–º–µ –º–µ—Ç–∫–∏ FLAG)
X_full = df_clean.drop(columns=["FLAG"]).values

# 2) –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –ø–æ–¥–≥–æ–Ω—è–µ–º scaler –Ω–∞ –≤—Å—ë–º X_full
scaler = MinMaxScaler()
scaler.fit(X_full)

# 3) –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
X_train_scaled = scaler.transform(X_train_res)  # resampled train
X_test_scaled = scaler.transform(X_test)       # original test

# 4) –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
print("== X_train_scaled ==")
print("min:", np.min(X_train_scaled), " max:", np.max(X_train_scaled))
print("shape:", X_train_scaled.shape)

print("\n== X_test_scaled ==")
print("min:", np.min(X_test_scaled), " max:", np.max(X_test_scaled))
print("shape:", X_test_scaled.shape)

# %%
# === @title –≠—Ç–∞–ø 7: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM ===

# 1) –§–æ—Ä–º–∞ –¥–ª—è LSTM: (samples, timesteps=1, features)
n_features = X_train_scaled.shape[1]

X_train_lstm = X_train_scaled.reshape(-1, 1, n_features)
X_test_lstm = X_test_scaled.reshape(-1, 1, n_features)

# 2) –¶–µ–ª–µ–≤—ã–µ –º–∞—Å—Å–∏–≤—ã
y_train_lstm = y_train_res.values if hasattr(
    y_train_res, "values") else np.array(y_train_res)
y_test_lstm = y_test.values if hasattr(y_test, "values") else np.array(y_test)

# 3) –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
print("X_train_lstm shape:", X_train_lstm.shape)
print("X_test_lstm  shape:", X_test_lstm.shape)
print("y_train_lstm shape:", y_train_lstm.shape)
print("y_test_lstm  shape:", y_test_lstm.shape)

# %%
# === @title –≠—Ç–∞–ø 8: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ LSTM-–º–æ–¥–µ–ª–∏ ===

# 1) –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
model = Sequential([
    # timesteps=1, features=n_features
    LSTM(64, input_shape=(1, X_train_lstm.shape[2])),
    Dense(1, activation='sigmoid')
])

# 2) –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 3) –í—ã–≤–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
model.summary()

# %%
# === @title –≠—Ç–∞–ø 9: –æ–±—É—á–µ–Ω–∏–µ LSTM-–º–æ–¥–µ–ª–∏ ===

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
epochs = 200
batch_size = 64

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
history = model.fit(
    X_train_lstm, y_train_lstm,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=0.20,
    shuffle=True,
    verbose=2
)

# --- –û—Ç–ª–∞–¥–æ—á–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ ---

# 1) Loss
plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'],    label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Training & Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# 2) Accuracy
plt.figure(figsize=(8, 4))
plt.plot(history.history['accuracy'],    label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.title('Training & Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# 3) –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
print(f"Final train_acc: {history.history['accuracy'][-1]:.4f}")
print(f"Final val_acc:   {history.history['val_accuracy'][-1]:.4f}")

# %%
# === @title –≠—Ç–∞–ø 10: –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–µ ===

# 1) –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred_proba = model.predict(X_test_lstm, verbose=0)
y_pred = (y_pred_proba >= 0.5).astype(int).flatten()

# 2) –ú–µ—Ç—Ä–∏–∫–∏
acc = accuracy_score(y_test_lstm, y_pred)
prec = precision_score(y_test_lstm, y_pred)
rec = recall_score(y_test_lstm, y_pred)
f1 = f1_score(y_test_lstm, y_pred)
cm = confusion_matrix(y_test_lstm, y_pred)

# 3) –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("=== Test Metrics ===")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}\n")

print("=== Confusion Matrix ===")
print(cm)

# %%
# –®–∞–≥ 1: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤

# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —á–µ–∫–ø–æ–π–Ω—Ç–æ–≤, –µ—Å–ª–∏ –µ—ë –µ—â—ë –Ω–µ—Ç
os.makedirs("checkpoints", exist_ok=True)

# –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤
checkpoint_path = "checkpoints/lstm_best_weights.h5"

# Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ 'val_loss'
cp_callback = ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',       # –°–ª–µ–¥–∏–º –∑–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π
    save_best_only=True,      # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–µ –≤–µ—Å–∞
    mode='min',               # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è val_loss)
    verbose=1,                # –ü–µ—á–∞—Ç—å, –∫–æ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
    save_freq='epoch'        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
)

# –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–æ–≥–æ –∫–æ–ª–±—ç–∫–∞
history = model.fit(
    X_train_lstm, y_train_lstm,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=0.20,
    shuffle=True,
    callbacks=[cp_callback],  # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–±—ç–∫
    verbose=2
)

# –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ü–µ–ª–∏–∫–æ–º –∏ –≤–µ—Å–∞

# 1) –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
artifacts_dir = Path("artifacts")
artifacts_dir.mkdir(exist_ok=True)

# 2) –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å —Ü–µ–ª–∏–∫–æ–º (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ + –≤–µ—Å–∞)
model_path = artifacts_dir / "lstm_eth_fraud_model.h5"
model.save(str(model_path))
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")

# 3) –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
scaler_path = artifacts_dir / "scaler.pkl"
with open(scaler_path, "wb") as f:
    pickle.dump(scaler, f)
print(f"Scaler —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {scaler_path}")

# 4) –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—á—Ç–æ–±—ã –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∑–Ω–∞—Ç—å –ø–æ—Ä—è–¥–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤)
feature_list = X.columns.tolist()  # X ‚Äì –¥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è, –±–µ–∑ FLAG
features_path = artifacts_dir / "features_list.pkl"
with open(features_path, "wb") as f:
    pickle.dump(feature_list, f)
print(f"–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {features_path}")

# –®–∞–≥ 4: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

# –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ –ø–µ—Ä–≤–æ–≥–æ LSTM —Å–ª–æ—è)
embedding_model = tf.keras.Model(
    inputs=model.input, outputs=model.layers[0].output)

# –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
embeddings_train = embedding_model.predict(X_train_lstm)
embeddings_test = embedding_model.predict(X_test_lstm)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
np.save("artifacts/embeddings_train.npy", embeddings_train)
np.save("artifacts/embeddings_test.npy", embeddings_test)
print(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'artifacts/embeddings_train.npy' –∏ 'artifacts/embeddings_test.npy'")

# –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ (–µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)
# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –¥—Ä—É–≥–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
model = tf.keras.models.load_model(
    str(artifacts_dir / "lstm_eth_fraud_model.h5"))
with open(artifacts_dir / "scaler.pkl", "rb") as f:
    scaler = pickle.load(f)
with open(artifacts_dir / "features_list.pkl", "rb") as f:
    features_list = pickle.load(f)

# –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
embeddings_train = np.load("artifacts/embeddings_train.npy")
embeddings_test = np.load("artifacts/embeddings_test.npy")

print("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.")
